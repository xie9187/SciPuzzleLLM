import pandas as pd
import random
import os
from os.path import join

from pymatgen.core.periodic_table import Element
from sklearn.preprocessing import MinMaxScaler

from table_agents_v2 import *
from data_utils import *
from code_executor import enhanced_code_execution, only_code_execution


def get_element_group(symbol):
    """
    è·å–å…ƒç´ çš„åŒ–å­¦æ—ä¿¡æ¯
    åŸºäºPeriodicTable_mjlf.csvçš„æ•°æ®
    """
    group_mapping = {
        # ç¬¬1æ— - ç¢±é‡‘å±
        "Li": "Alkali metal", "Na": "Alkali metal", "K": "Alkali metal", "Rb": "Alkali metal", "Cs": "Alkali metal",
        # ç¬¬2æ— - ç¢±åœŸé‡‘å±
        "Be": "Alkaline earth", "Mg": "Alkaline earth", "Ca": "Alkaline earth", "Sr": "Alkaline earth", "Ba": "Alkaline earth",
        # ç¬¬13æ— - ç¡¼æ—
        "B": "Boron group", "Al": "Boron group", "Ga": "Boron group", "In": "Boron group", "Tl": "Boron group",
        # ç¬¬14æ— - ç¢³æ—
        "C": "Carbon", "Si": "Carbon", "Ge": "Carbon", "Sn": "Carbon", "Pb": "Carbon",
        # ç¬¬15æ— - æ°®æ—
        "N": "Nitrogen", "P": "Nitrogen", "As": "Nitrogen", "Sb": "Nitrogen", "Bi": "Nitrogen",
        # ç¬¬16æ— - æ°§æ—ï¼ˆç¡«æ—ï¼‰
        "O": "Chalcogen", "S": "Chalcogen", "Se": "Chalcogen", "Te": "Chalcogen", "Po": "Chalcogen",
        # ç¬¬17æ— - å¤ç´ 
        "F": "Halogen", "Cl": "Halogen", "Br": "Halogen", "I": "Halogen", "At": "Halogen",
        # ç¬¬18æ— - æƒ°æ€§æ°”ä½“
        "He": "Noble gas", "Ne": "Noble gas", "Ar": "Noble gas", "Kr": "Noble gas", "Xe": "Noble gas", "Rn": "Noble gas",
        # è¿‡æ¸¡é‡‘å±æ—
        "Sc": "Transition metal", "Ti": "Transition metal", "V": "Transition metal", "Cr": "Transition metal", "Mn": "Transition metal",
        "Fe": "Iron", "Co": "Iron", "Ni": "Iron", "Cu": "Copper", "Zn": "Zinc",
        "Y": "Transition metal", "Zr": "Transition metal", "Nb": "Transition metal", "Mo": "Transition metal", "Tc": "Transition metal",
        "Ru": "Platinum", "Rh": "Platinum", "Pd": "Platinum", "Ag": "Copper", "Cd": "Zinc",
        "La": "Lanthanide", "Ce": "Lanthanide", "Pr": "Lanthanide", "Nd": "Lanthanide", "Pm": "Lanthanide",
        "Sm": "Lanthanide", "Eu": "Lanthanide", "Gd": "Lanthanide", "Tb": "Lanthanide", "Dy": "Lanthanide",
        "Ho": "Lanthanide", "Er": "Lanthanide", "Tm": "Lanthanide", "Yb": "Lanthanide", "Lu": "Lanthanide",
        "Hf": "Transition metal", "Ta": "Transition metal", "W": "Transition metal", "Re": "Transition metal", "Os": "Transition metal",
        "Ir": "Transition metal", "Pt": "Platinum", "Au": "Transition metal", "Hg": "Zinc", "Tl": "Transition metal",
        "Pb": "Carbon", "Bi": "Nitrogen", "Po": "Chalcogen", "At": "Halogen", "Rn": "Noble gas",
        # ç‰¹æ®Šå…ƒç´ 
        "H": "Hydrogen"
    }
    return group_mapping.get(symbol, "unknow")

def get_elemental_affinity(symbol):
    """
    è·å–å…ƒç´ çš„äº²å’Œæ€§ä¿¡æ¯
    åŸºäºPeriodicTable_mjlf.csvçš„æ•°æ®
    """
    affinity_mapping = {
        # äº²çŸ³å…ƒç´  (Lithophile)
        "Li": "Lithophile", "Na": "Lithophile", "K": "Lithophile", "Rb": "Lithophile", "Cs": "Lithophile",
        "Be": "Lithophile", "Mg": "Lithophile", "Ca": "Lithophile", "Sr": "Lithophile", "Ba": "Lithophile",
        "B": "Lithophile", "Al": "Lithophile", "Ga": "Lithophile", "In": "Lithophile", "Tl": "Lithophile",
        "Si": "Lithophile", "Ge": "Lithophile", "Sn": "Lithophile", "Pb": "Lithophile",
        "P": "Lithophile", "As": "Lithophile", "Sb": "Nitrogen", "Bi": "Nitrogen",
        "S": "Chalcophile", "Se": "Chalcophile", "Te": "Chalcophile", "Po": "Chalcogen",
        "F": "Lithophile", "Cl": "Lithophile", "Br": "Lithophile", "I": "Lithophile", "At": "Halogen",
        "Sc": "Lithophile", "Ti": "Lithophile", "V": "Lithophile", "Cr": "Lithophile", "Mn": "Siderophile",
        "Fe": "Siderophile", "Co": "Siderophile", "Ni": "Siderophile", "Cu": "Chalcophile", "Zn": "Chalcophile",
        "Y": "Lithophile", "Zr": "Lithophile", "Nb": "Lithophile", "Mo": "Siderophile", "Tc": "unknow",
        "Ru": "Siderophile", "Rh": "Siderophile", "Pd": "Siderophile", "Ag": "Chalcophile", "Cd": "Chalcophile",
        "La": "Lithophile", "Ce": "Lithophile", "Pr": "Lithophile", "Nd": "Lithophile", "Pm": "Lanthanide",
        "Sm": "Lanthanide", "Eu": "Lanthanide", "Gd": "Lanthanide", "Tb": "Lanthanide", "Dy": "Lanthanide",
        "Ho": "Lanthanide", "Er": "Lanthanide", "Tm": "Lanthanide", "Yb": "Lanthanide", "Lu": "Lanthanide",
        "Hf": "Lithophile", "Ta": "Lithophile", "W": "Lithophile", "Re": "Lithophile", "Os": "Siderophile",
        "Ir": "Siderophile", "Pt": "Siderophile", "Au": "Chalcophile", "Hg": "Chalcophile", "Tl": "Chalcophile",
        # äº²æ°”å…ƒç´  (Atmophile)
        "H": "Atmophile", "He": "Atmophile", "Ne": "Atmophile", "Ar": "Atmophile", "Kr": "Atmophile", "Xe": "Atmophile",
        "N": "Atmophile", "O": "Lithophile", "C": "Atmophile"
    }
    return affinity_mapping.get(symbol, "unknow")

def get_properties_of_oxides(symbol):
    """
    è·å–å…ƒç´ çš„æ°§åŒ–ç‰©æ€§è´¨ä¿¡æ¯
    åŸºäºPeriodicTable_mjlf.csvçš„æ•°æ®
    """
    oxide_mapping = {
        # å¼ºç¢±æ€§æ°§åŒ–ç‰©
        "Li": "Strongly basic oxide", "Na": "Strongly basic oxide", "K": "Strongly basic oxide", 
        "Rb": "Strongly basic oxide", "Cs": "Strongly basic oxide",
        "Be": "Amphoteric oxide", "Mg": "Basic oxide", "Ca": "Strongly basic oxide", 
        "Sr": "Strongly basic oxide", "Ba": "Strongly basic oxide",
        # ä¸¤æ€§æ°§åŒ–ç‰©
        "Al": "Amphoteric oxide", "Ga": "Amphoteric oxide", "In": "Amphoteric oxide", "Tl": "Amphoteric oxide",
        "Si": "Weakly acidic oxide", "Ge": "Amphoteric oxide", "Sn": "Amphoteric oxide", "Pb": "Amphoteric oxide",
        "Ti": "Amphoteric oxide", "V": "Acidic oxide", "Cr": "Amphoteric oxide", "Mn": "Strongly acidic oxide",
        "Fe": "Weakly basic oxide", "Co": "Basic oxide", "Ni": "Basic oxide", "Cu": "Weakly basic oxide", "Zn": "Amphoteric oxide",
        "Zr": "Amphoteric oxide", "Nb": "Weakly acidic oxide", "Mo": "Acidic oxide", "Tc": "Strongly acidic oxide",
        "Ru": "Weakly basic oxide", "Rh": "Weakly basic oxide", "Pd": "Basic oxide", "Ag": "Weakly basic oxide", "Cd": "Basic oxide",
        "Hf": "Amphoteric oxide", "Ta": "Weakly acidic oxide", "W": "Weakly acidic oxide", "Re": "Weakly acidic oxide", "Os": "Weakly basic oxide",
        "Ir": "Weakly basic oxide", "Pt": "Weakly basic oxide", "Au": "Weakly basic oxide", "Hg": "Weakly basic oxide",
        # é…¸æ€§æ°§åŒ–ç‰©
        "B": "Weakly acidic oxide", "C": "Weakly acidic oxide", "N": "Acidic oxide", "P": "Acidic oxide", "As": "Acidic oxide", "Sb": "Acidic oxide", "Bi": "Acidic oxide",
        "S": "Strongly acidic oxide", "Se": "Weakly acidic oxide", "Te": "Acidic oxide", "Po": "Acidic oxide",
        "F": "Strongly oxidizing oxide", "Cl": "Strongly acidic oxide", "Br": "Strongly acidic oxide", "I": "Strongly acidic oxide", "At": "Strongly acidic oxide",
        "V": "Acidic oxide", "Cr": "Amphoteric oxide", "Mn": "Strongly acidic oxide", "Mo": "Acidic oxide", "Tc": "Strongly acidic oxide",
        # ä¸­æ€§æ°§åŒ–ç‰©
        "H": "Neutral oxide", "He": "unknow", "Ne": "unknow", "Ar": "unknow", "Kr": "unknow", "Xe": "unknow", "Rn": "unknow",
        # é•§ç³»å…ƒç´ 
        "La": "Strongly basic oxide", "Ce": "Weakly acidic oxide", "Pr": "Basic oxide", "Nd": "Basic oxide", "Pm": "Basic oxide",
        "Sm": "Basic oxide", "Eu": "Basic oxide", "Gd": "Basic oxide", "Tb": "Basic oxide", "Dy": "Basic oxide",
        "Ho": "Basic oxide", "Er": "Basic oxide", "Tm": "Basic oxide", "Yb": "Basic oxide", "Lu": "Basic oxide"
    }
    return oxide_mapping.get(symbol, "unknow")

def parse_electronic_structure(electronic_structure):
    """
    è§£æç”µå­ç»“æ„å­—ç¬¦ä¸²ï¼Œæå–ä»·å±‚ä¿¡æ¯
    ä¾‹å¦‚: "[Ne].3s2.2p3" -> ä»·å±‚ç¼–å·=3, ä»·ç”µå­æ•°=5
    """
    if not electronic_structure:
        return None, None, None
    
    # ç§»é™¤ç¨€æœ‰æ°”ä½“ç¬¦å·ï¼Œå¦‚ [Ne], [Ar] ç­‰
    parts = electronic_structure.split('.')
    
    # æ‰¾åˆ°æœ€é«˜ä¸»é‡å­æ•°ï¼ˆä»·å±‚ç¼–å·ï¼‰
    max_n = 0
    valence_electrons = 0
    
    for part in parts:
        if part.startswith('[') and part.endswith(']'):
            continue  # è·³è¿‡ç¨€æœ‰æ°”ä½“ç¬¦å·
        
        # è§£æå¦‚ "3s2", "2p3" ç­‰
        if len(part) >= 2:
            try:
                n = int(part[0])  # ä¸»é‡å­æ•°
                max_n = max(max_n, n)
                
                # è®¡ç®—ç”µå­æ•°
                if len(part) > 2:
                    electron_count = int(part[2:])
                    valence_electrons += electron_count
                else:
                    valence_electrons += 1
            except (ValueError, IndexError):
                continue
    
    return max_n, valence_electrons

def generate_table():
    # åˆå§‹åŒ–æ•°æ®åˆ—è¡¨
    element_data = []

    # é—¨æ·åˆ—å¤«å½“æ—¶å·²çŸ¥å…ƒç´ 
    mendeleev_known_elements = {
        1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20,
        21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 37, 38,
        39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56,
        57, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 92
    }

    # å…ˆåªè€ƒè™‘ä½¿ç”¨å‰60ä¸ªå…ƒç´ ï¼Œé™ä½éš¾åº¦
    n_elem = 60
    
    # éšæœºé€‰æ‹©10ä¸ªæµ‹è¯•é›†å…ƒç´ 
    test = random.sample(range(n_elem), 10)
    
    for Z in range(1, n_elem+1):
        e = Element.from_Z(Z)

        # é‡‘å±æ€§è´¨åˆ¤æ–­ï¼ˆåªç”¨æ–‡æ¡£ä¸­å®é™…å­˜åœ¨çš„å­—æ®µï¼‰
        if e.is_metal:
            metal_type = "Metal"
        elif e.is_metalloid:
            metal_type = "Metalloid"
        # elif e.is_noble_gas or e.is_halogen or e.is_chalcogen:
        #     metal_type = "Nonmetal"
        else:
            metal_type = "Nonmetal"


        # å¸¸æ¸©çŠ¶æ€åˆ¤æ–­ï¼ˆç”¨ boiling_point å’Œ melting_pointï¼‰
        if e.boiling_point and e.boiling_point < 298:
            state = "Gas"
        elif e.melting_point and e.melting_point < 298 < (e.boiling_point or 9999):
            state = "Liquid"
        else:
            state = "Solid"

        # è·å–ç”µå­ç»“æ„ä¿¡æ¯
        # Block: ç”µå­äºšå±‚ç±»å‹ (s, p, d, f)
        block = e.block
        
        # ValenceShellN: ä»·å±‚ç¼–å·ï¼ˆæœ€å¤–å±‚ç”µå­å±‚çš„ä¸»é‡å­æ•°ï¼‰
        # ValenceElectrons: ä»·ç”µå­æ•°ï¼ˆæœ€å¤–å±‚ç”µå­æ•°ï¼‰
        valence_shell_n, valence_electrons = parse_electronic_structure(e.electronic_structure)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ³•
        if valence_shell_n is None:
            valence_shell_n = e.row  # ä½¿ç”¨å‘¨æœŸæ•°ä½œä¸ºå¤‡é€‰
            valence_electrons = sum(e.valence) if e.valence else 0  # ä½¿ç”¨åŒ–åˆä»·ä½œä¸ºå¤‡é€‰

        element_data.append({
            "AtomicNumber": e.Z,
            "Element": e.long_name,
            "Symbol": e.symbol,
            "AtomicMass": float(e.atomic_mass) if e.atomic_mass else None,
            "OxidationStates": ", ".join(map(str, e.common_oxidation_states)) if e.common_oxidation_states else 0,
            "StateAtRoomTemp": state,
            "MetalType": metal_type,
            "Block": block,
            "ValenceShellN": valence_shell_n,
            "ValenceElectrons": valence_electrons,
            "Test": e.Z in test
        })

    df = pd.DataFrame(element_data)

    most_common_oxidation_states = {
        # ç¬¬ä¸€å‘¨æœŸ
        "H": +1, "He": 0,
        # ç¬¬äºŒå‘¨æœŸ
        "Li": +1, "Be": +2, "B": +3, "C": +4, "N": -3, "O": -2, "F": -1, "Ne": 0,
        # ç¬¬ä¸‰å‘¨æœŸ
        "Na": +1, "Mg": +2, "Al": +3, "Si": +4, "P": +5, "S": -2, "Cl": -1, "Ar": 0,
        # ç¬¬å››å‘¨æœŸ
        "K": +1, "Ca": +2, "Sc": +3, "Ti": +4, "V": +5, "Cr": +3, "Mn": +2,
        "Fe": +3, "Co": +2, "Ni": +2, "Cu": +2, "Zn": +2, "Ga": +3, "Ge": +4,
        "As": +5, "Se": -2, "Br": -1, "Kr": 0,
        # ç¬¬äº”å‘¨æœŸ
        "Rb": +1, "Sr": +2, "Y": +3, "Zr": +4, "Nb": +5, "Mo": +6, "Tc": +7,
        "Ru": +3, "Rh": +3, "Pd": +2, "Ag": +1, "Cd": +2, "In": +3, "Sn": +4,
        "Sb": +3, "Te": -2, "I": -1, "Xe": 0,
        # ç¬¬å…­å‘¨æœŸ
        "Cs": +1, "Ba": +2, "La": +3, "Ce": +4, "Pr": +3, "Nd": +3, "Pm": +3,
        "Sm": +3, "Eu": +3, "Gd": +3, "Tb": +3, "Dy": +3, "Ho": +3, "Er": +3,
        "Tm": +3, "Yb": +3, "Lu": +3,
        "Hf": +4, "Ta": +5, "W": +6, "Re": +7, "Os": +4, "Ir": +3, "Pt": +2,
        "Au": +3, "Hg": +2, "Tl": +3, "Pb": +2, "Bi": +3, "Po": +2, "At": -1,
        "Rn": 0,
        # ç¬¬ä¸ƒå‘¨æœŸ
        "Fr": +1, "Ra": +2, "Ac": +3, "Th": +4, "Pa": +5, "U": +6, "Np": +5,
        "Pu": +4, "Am": +3, "Cm": +3, "Bk": +3, "Cf": +3, "Es": +3, "Fm": +3,
        "Md": +3, "No": +2, "Lr": +3,
        "Rf": +4, "Db": +5, "Sg": +6, "Bh": +7, "Hs": +4, "Mt": +1,
        "Ds": +0, "Rg": +1, "Cn": +2, "Nh": +1, "Fl": +2, "Mc": +1,
        "Lv": +2, "Ts": -1, "Og": 0
    }
    df["OxidationStates"] = df["Symbol"].map(most_common_oxidation_states)

    return df

def generate_test_column(df: pd.DataFrame, num_test_elements: int = 10) -> pd.DataFrame:
    """
    ä¸ºDataFrameæ·»åŠ Teståˆ—ï¼Œéšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„å…ƒç´ ä½œä¸ºæµ‹è¯•é›†
    
    å‚æ•°:
        df: è¾“å…¥çš„DataFrame
        num_test_elements: è¦é€‰æ‹©ä¸ºæµ‹è¯•é›†çš„å…ƒç´ æ•°é‡ï¼Œé»˜è®¤10ä¸ª
    
    è¿”å›:
        æ·»åŠ äº†Teståˆ—çš„DataFrame
    """
    # ç¡®ä¿æµ‹è¯•å…ƒç´ æ•°é‡ä¸è¶…è¿‡æ€»å…ƒç´ æ•°é‡
    num_test_elements = min(num_test_elements, len(df))
    
    # éšæœºé€‰æ‹©æµ‹è¯•å…ƒç´ çš„ç´¢å¼•
    test_indices = random.sample(range(len(df)), num_test_elements)
    
    # åˆå§‹åŒ–Teståˆ—ä¸ºFalse
    df = df.copy()
    df['Test'] = False
    
    # å°†é€‰ä¸­çš„å…ƒç´ æ ‡è®°ä¸ºTrue
    df.loc[test_indices, 'Test'] = True
      
    return df

def mask_table(df: pd.DataFrame, known_to_mendeleev: bool = True):
    # 0. ç”ŸæˆTeståˆ—ï¼Œéšæœºé€‰æ‹©10ä¸ªå…ƒç´ ä½œä¸ºæµ‹è¯•é›†
    df = generate_test_column(df, num_test_elements=10)
    
    # 1. å‰”é™¤ AtomicNumber å’Œ Symbol åˆ—
    df = df.drop(columns=["AtomicNumber", "Symbol"])

    # 2. ç”Ÿæˆ 3 ä½éšæœºä¸”ä¸é‡å¤çš„æ•°å­—ï¼Œç”¨äºæ›¿æ¢å…ƒç´ åç§°
    num_elements = len(df)
    random_numbers = random.sample(range(100, 1000), num_elements)  # ç”Ÿæˆ 3 ä½ä¸é‡å¤éšæœºæ•°
    df["Element"] = [f"Elem{num}" for num in random_numbers]

    # # 3. å°† AtomicMass å½’ä¸€åŒ–åˆ° [1, 100]
    # scaler = MinMaxScaler(feature_range=(1, 100))
    # df["AtomicMass"] = scaler.fit_transform(df[["AtomicMass"]])

    # 5. StateAtRoomTemp æ˜ å°„ä¸º State1, State2...ï¼ˆéšæœºé¡ºåºï¼Œæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼‰
    if "StateAtRoomTemp" in df.columns:
        unique_states = [state for state in df["StateAtRoomTemp"].unique() if state != "unknow"]
        # éšæœºæ‰“ä¹±
        random.shuffle(unique_states)
        state_mapping = {state: f"State{i+1}" for i, state in enumerate(unique_states)}
        # ä¿æŒunknowä¸å˜
        if "unknow" in df["StateAtRoomTemp"].unique():
            state_mapping["unknow"] = "unknow"
        df["StateAtRoomTemp"] = df["StateAtRoomTemp"].map(state_mapping)
        

    # 6. MetalType æ˜ å°„ä¸º Type1, Type2...ï¼ˆéšæœºé¡ºåºï¼Œæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼‰
    if "MetalType" in df.columns:
        unique_types = [typ for typ in df["MetalType"].unique() if typ != "unknow"]
        random.shuffle(unique_types)
        type_mapping = {typ: f"Type{i+1}" for i, typ in enumerate(unique_types)}
        # ä¿æŒunknowä¸å˜
        if "unknow" in df["MetalType"].unique():
            type_mapping["unknow"] = "unknow"
        df["MetalType"] = df["MetalType"].map(type_mapping)
        
    # 7. TheGroup æ˜ å°„ä¸º Group1, Group2...ï¼ˆéšæœºé¡ºåºï¼Œä¿æŒunknowä¸å˜ï¼Œæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼‰
    if "TheGroup" in df.columns:
        group_values = [group for group in df["TheGroup"].unique() if group != "unknow"]
        random.shuffle(group_values)
        group_mapping = {group: f"Group{i+1}" for i, group in enumerate(group_values)}
        # ä¿æŒunknowä¸å˜
        if "unknow" in df["TheGroup"].unique():
            group_mapping["unknow"] = "unknow"
        df["TheGroup"] = df["TheGroup"].map(group_mapping)
        

    # 8. ElementalAffinity æ˜ å°„ä¸º Affinity1, Affinity2...ï¼ˆéšæœºé¡ºåºï¼Œä¿æŒunknowä¸å˜ï¼Œæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼‰
    if "ElementalAffinity" in df.columns:
        affinity_values = [affinity for affinity in df["ElementalAffinity"].unique() if affinity != "unknow"]
        random.shuffle(affinity_values)
        affinity_mapping = {affinity: f"Affinity{i+1}" for i, affinity in enumerate(affinity_values)}
        # ä¿æŒunknowä¸å˜
        if "unknow" in df["ElementalAffinity"].unique():
            affinity_mapping["unknow"] = "unknow"
        df["ElementalAffinity"] = df["ElementalAffinity"].map(affinity_mapping)
        

    # 9. PropertiesOfOxides æ˜ å°„ä¸º Oxide1, Oxide2...ï¼ˆéšæœºé¡ºåºï¼Œä¿æŒunknowä¸å˜ï¼Œæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼‰
    """ if "PropertiesOfOxides" in df.columns:
        oxide_values = [oxide for oxide in df["PropertiesOfOxides"].unique() if oxide != "unknow"]
        random.shuffle(oxide_values)
        oxide_mapping = {oxide: f"Oxide{i+1}" for i, oxide in enumerate(oxide_values)}
        # ä¿æŒunknowä¸å˜
        if "unknow" in df["PropertiesOfOxides"].unique():
            oxide_mapping["unknow"] = "unknow"
        df["PropertiesOfOxides"] = df["PropertiesOfOxides"].map(oxide_mapping)
         """

    # 10. è½¬æ¢åˆ—åä¸º Attribute1, Attribute2,...
    columns = list(df.columns)
    
    # ç¬¬ä¸€åˆ—å‘½åä¸º Element
    new_column_names = {columns[0]: "Element"}
    
    # å…¶ä½™åˆ—å‘½åä¸º Attribute1, Attribute2, ...
    # åœ¨é‡å‘½åå‰ï¼ŒåŸºäºå½“å‰åˆ—é¡ºåºæ„å»º attr_name_mapï¼ˆAttributeX -> åŸå§‹åˆ—åï¼‰
    attr_name_map = {}
    for i, col in enumerate(columns[1:-1], start=1):
        new_name = f"Attribute{i}"
        new_column_names[col] = new_name
        attr_name_map[new_name] = col
    
    df = df.rename(columns=new_column_names)

    # 8. æŒ‰ç…§ Element æ’åºï¼ˆæŒ‰ ElemXXX çš„æ•°å­—éƒ¨åˆ†å‡åºæ’åˆ—ï¼‰
    df["TempSortKey"] = df["Element"].str.extract(r"(\d+)").astype(int)
    df = df.sort_values("TempSortKey").drop(columns="TempSortKey")

    # 9. æ ¹æ® Known åˆ†å‰²æ•°æ®
    train_df = df[df["Test"] == False].drop(columns=["Test"])
    test_df = df[df["Test"] == True].drop(columns=["Test"])

    # è¿”å›è®­ç»ƒé›†ã€æµ‹è¯•é›†ï¼Œä»¥åŠå±æ€§æ˜ å°„è¡¨ï¼ˆç”¨äºåŒ¹é…åŠ æƒï¼‰
    return train_df, test_df, attr_name_map

class TableState(object):
    def __init__(self, elem_df, test_df):
        super(TableState, self).__init__()
        self.elem_df = elem_df
        self.elem_attrs = list(elem_df.columns)
        #self.elem_df['row'] = None
        #self.elem_df['col'] = None
        if 'row' not in self.elem_df.columns:
            self.elem_df['row'] = None
        if 'col' not in self.elem_df.columns:
            self.elem_df['col'] = None
        self.test_df = test_df

    def fill_elem_posi(self, actions):
        for elem, row, col in actions:
            self.elem_df.loc[elem, ['row', 'col']] = [row, col]

    def append_new_elem(self, elems):
        for elem_attr in elems:
            # elem_attr = [str(val) for val in elem_attr]
            self.elem_df.loc[elem_attr[0]] = elem_attr[1:] 
        
    def sort_table(self, sort_col, ascending=True):
        self.raw_df = self.elem_df.copy()
        self.elem_df = self.elem_df.sort_values(sort_col, ascending=ascending)

    def state_as_long_str(self, df=None):
        if df is None:
            return self.elem_df.to_string(index=True)
        else:
            return df.to_string(index=True)

    def att_table(self, attr_name):
        """
        ç”Ÿæˆå±æ€§å€¼çš„è¡¨æ ¼å¯è§†åŒ–å­—ç¬¦ä¸²
        å‚æ•°:
            attr_name: è¦æ˜¾ç¤ºçš„å±æ€§åˆ—å
        è¿”å›:
            å¯¹é½çš„è¡¨æ ¼å­—ç¬¦ä¸²ï¼Œç©ºç™½ä½ç½®ç”¨ç©ºå­—ç¬¦ä¸²è¡¨ç¤º
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒç´ å·²å¡«å…¥è¡¨æ ¼
        if self.elem_df[['col', 'row']].isna().all().all():
            return 'No element in table currently.'
        
        # è·å–è¡¨æ ¼çš„è¡Œåˆ—èŒƒå›´
        max_row = int(self.elem_df['row'].max())
        max_col = int(self.elem_df['col'].max())
        
        # åˆ›å»ºç©ºè¡¨æ ¼ (row+1è¡Œ x col+1åˆ—ï¼Œå› ä¸ºä»0æˆ–1å¼€å§‹è®¡æ•°)
        table = [['' for _ in range(max_col)] for _ in range(max_row)]
        
        # å¡«å……å·²å¡«å…¥çš„å…ƒç´ 
        filled = self.elem_df.dropna(subset=['col', 'row'])
        for _, row in filled.iterrows():
            r = int(row['row']) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
            c = int(row['col']) - 1
            if 0 <= r < max_row and 0 <= c < max_col:
                attr_value = row[attr_name]
                if pd.api.types.is_float(attr_value) and not pd.isna(attr_value):
                    table[r][c] = f"{float(attr_value):.1f}"
                else:
                    table[r][c] = str(attr_value)
        
        # è®¡ç®—æ¯åˆ—æœ€å¤§å®½åº¦
        col_widths = [max(len(str(table[r][c])) for r in range(max_row)) for c in range(max_col)]
        
        # ç”Ÿæˆè¡¨æ ¼å­—ç¬¦ä¸²
        table_lines = []
        for r in range(max_row):
            line = '|' + ', '.join(
                f" {table[r][c]:^{col_widths[c]}} " for c in range(max_col)
            ) + '|'
            table_lines.append(line)
        
        return '\n'.join(table_lines)

    def find_matched_elements(self, df1, df2, tolerance=1.0, attr_name_map=None):
        """
        åœ¨åŒ¿ååŒ–åˆ—ååœºæ™¯ä¸‹ï¼ŒåŒ¹é… df1 ä¸ df2 çš„å…ƒç´ è¡Œï¼Œè¿”å›å¸¦æœ‰åŒ¹é…åˆ†æ•°çš„å­é›†ã€‚
        - è‹¥æä¾› attr_name_mapï¼ˆåˆ—å -> åŸå§‹å±æ€§åï¼‰ï¼ŒæŒ‰æ–¹æ¡ˆBå¯¹ä¸åŒå±æ€§åŠ æƒï¼›
          å¦åˆ™å¯¹æ‰€æœ‰å‚ä¸å±æ€§ç­‰æƒé‡å¤„ç†ã€‚
        - ä¸åŒæ¨¡å¼ä¸‹å±æ€§æ•°é‡å¯èƒ½ä¸åŒï¼›æœ¬å‡½æ•°ä¼šå¯¹å‚ä¸è¯„åˆ†çš„å±æ€§æƒé‡å½’ä¸€åŒ–ï¼Œä½¿æ»¡åˆ†æ’ä¸º1.0ã€‚

        å‚æ•°:
            df1: å¾…åŒ¹é…çš„ DataFrameï¼ˆå¯èƒ½åŒ…å« row å’Œ colï¼‰
            df2: ç›®æ ‡ DataFrame
            tolerance: æ•°å€¼å±æ€§å…è®¸çš„è¯¯å·®èŒƒå›´ï¼ˆé»˜è®¤ 1.0ï¼‰
            attr_name_map: å¯é€‰ï¼Œdict æ˜ å°„ {åˆ—å: åŸå§‹å±æ€§å}ï¼Œç”¨äºæŒ‰æ–¹æ¡ˆBåŠ æƒã€‚

        è¿”å›:
            DataFrame: åŒ…å« df1 ä¸­æ¯è¡Œçš„æœ€ä½³åŒ¹é…åˆ†æ•°ï¼ˆmatch_scoreï¼‰ä¸å¯¹åº”ç›®æ ‡ç´¢å¼•ï¼ˆmatched_withï¼‰ã€‚
        """
        # å…±æœ‰åˆ—ï¼ˆæ’é™¤ row/colï¼‰
        common_cols = [col for col in df1.columns if col in df2.columns and col not in ["row", "col"]]
        if not common_cols:
            return pd.DataFrame()

        # æ–¹æ¡ˆBæƒé‡ï¼ˆåŸå§‹å±æ€§å -> æƒé‡ï¼‰
        scheme_b_weights = {
            'OxidationStates': 0.13,
            'PropertiesOfOxides': 0.1,
            'ElementalAffinity': 0.1,
            'MetalType': 0.1,
            'AtomicMass': 0.30,
            'StateAtRoomTemp': 0.07,
            'TheGroup': 0.2,
        }

        # åˆ¤æ–­æ˜¯å¦å¯ä»¥ä½¿ç”¨æ–¹æ¡ˆBåŠ æƒï¼šattr_name_map è¦†ç›–æ‰€æœ‰éœ€è¦å‚ä¸çš„åˆ—
        use_scheme_b = isinstance(attr_name_map, dict) and all(
            (col in attr_name_map and attr_name_map[col] in scheme_b_weights)
            for col in common_cols
        )

        result_data = []

        # éå† df1 çš„æ¯ä¸€è¡Œ
        for idx1, row1 in df1.iterrows():
            best_score = 0.0
            best_match_idx = None

            for idx2, row2 in df2.iterrows():
                # è®¡ç®—å¯ç”¨åˆ—ï¼ˆä¸¤ä¾§å‡éç¼ºå¤±/é 'unknow'ï¼‰
                considered_cols = []
                col_weights = []

                for col in common_cols:
                    v1 = row1[col]
                    v2 = row2[col]
                    if pd.isna(v1) or pd.isna(v2):
                        continue
                    if isinstance(v1, str) and v1 == 'unknow':
                        continue
                    if isinstance(v2, str) and v2 == 'unknow':
                        continue

                    considered_cols.append(col)
                    # è®¡ç®—è¯¥åˆ—çš„åŸºç¡€æƒé‡
                    if use_scheme_b:
                        canonical = attr_name_map[col]
                        col_weights.append(scheme_b_weights.get(canonical, 0.0))
                    else:
                        col_weights.append(1.0)  # ç­‰æƒï¼Œç¨åå½’ä¸€åŒ–

                if not considered_cols:
                    current_score = 0.0
                else:
                    # å½’ä¸€åŒ–åˆ—æƒé‡ï¼Œä½¿å…¶å’Œä¸º1
                    total_w = sum(col_weights)
                    if total_w <= 0:
                        # é€€åŒ–åˆ°ç­‰æƒ
                        col_weights = [1.0] * len(considered_cols)
                        total_w = float(len(considered_cols))
                    norm_weights = [w / total_w for w in col_weights]

                    # é€åˆ—è®¡ç®—åŒ¹é…å¾—åˆ†ï¼ˆæ•°å€¼å±æ€§ç”¨å®¹å·®ï¼Œç±»åˆ«å±æ€§ç”¨ç›¸ç­‰ï¼‰
                    current_score = 0.0
                    for col, w in zip(considered_cols, norm_weights):
                        v1 = row1[col]
                        v2 = row2[col]
                        # æ•°å€¼åŒ¹é…ï¼ˆå«æ•´å‹/æµ®ç‚¹ï¼‰
                        if pd.api.types.is_numeric_dtype(df1[col]) and pd.api.types.is_numeric_dtype(df2[col]):
                            try:
                                if abs(float(v1) - float(v2)) <= tolerance:
                                    current_score += w
                            except Exception:
                                # è§£æå¤±è´¥æ—¶é€€åŒ–ä¸ºä¸¥æ ¼ç›¸ç­‰
                                if v1 == v2:
                                    current_score += w
                        else:
                            # ç±»åˆ«åŒ¹é…
                            if v1 == v2:
                                current_score += w

                if current_score > best_score:
                    best_score = current_score
                    best_match_idx = idx2

            if best_match_idx is not None:
                new_row = row1.copy()
                new_row['match_score'] = float(best_score)
                new_row['matched_with'] = best_match_idx
                new_row['total_possible_score'] = 1.0
                result_data.append(new_row)

        return pd.DataFrame(result_data) if result_data else pd.DataFrame()
    
    def clear_new_elems(self):
        # æ¸…é™¤æ–°å…ƒç´ 
        self.elem_df = self.elem_df[~self.elem_df.index.str.startswith('NewElem')]
    
    def show_matching_details(self, matched_df, test_df):
        """
        æ˜¾ç¤ºåŒ¹é…è¯¦æƒ…ï¼ŒåŒ…æ‹¬æ¯ä¸ªæ–°å…ƒç´ çš„åŒ¹é…åˆ†æ•°å’ŒåŒ¹é…çš„çœŸå®å…ƒç´ 
        """
        if matched_df.empty:
            return "æ²¡æœ‰åŒ¹é…çš„å…ƒç´ "
        
        details = []
        for idx, row in matched_df.iterrows():
            if 'matched_with' in row and 'match_score' in row:
                matched_elem = row['matched_with']
                score = row['match_score']
                
                # è·å–åŒ¹é…çš„çœŸå®å…ƒç´ ä¿¡æ¯
                if matched_elem in test_df.index:
                    real_elem_info = test_df.loc[matched_elem]
                    details.append(f"{idx}: åŒ¹é… {matched_elem} (åˆ†æ•°: {score:.2f}/1.00)")
                else:
                    details.append(f"{idx}: åŒ¹é…åˆ†æ•° {score:.2f}/1.00 (ä½†åŒ¹é…å…ƒç´ ä¸å­˜åœ¨)")
            else:
                details.append(f"{idx}: æ— åŒ¹é…ä¿¡æ¯")
        
        return "\n".join(details)
    
    def rollback_to_state(self, target_table_state):
        """
        å›æ»šåˆ°æŒ‡å®šçš„è¡¨æ ¼çŠ¶æ€
        
        å‚æ•°:
            target_table_state: ç›®æ ‡è¡¨æ ¼çŠ¶æ€ï¼ˆTableStateå¯¹è±¡ï¼‰
        """
        if target_table_state is not None:
            # æ¢å¤è¡¨æ ¼æ•°æ®
            self.elem_df = target_table_state.elem_df.copy()
            # æ¢å¤å±æ€§åˆ—è¡¨
            self.elem_attrs = list(self.elem_df.columns)
            print("âœ… è¡¨æ ¼çŠ¶æ€å·²å›æ»š")
        else:
            print("âš ï¸  ç›®æ ‡çŠ¶æ€ä¸ºç©ºï¼Œæ— æ³•å›æ»š")
    
    def get_constraint_status(self):
        """
        è·å–å½“å‰è¡¨æ ¼çš„çº¦æŸçŠ¶æ€
        
        è¿”å›:
            dict: åŒ…å«å„ç§çº¦æŸæ£€æŸ¥ç»“æœçš„å­—å…¸
        """
        status = {
            'has_position_conflicts': False,
            'has_duplicate_positions': False,
            'has_invalid_positions': False,
            'constraint_violations': []
        }
        
        # æ£€æŸ¥ä½ç½®å†²çª
        if 'row' in self.elem_df.columns and 'col' in self.elem_df.columns:
            filled_elements = self.elem_df.dropna(subset=['row', 'col'])
            if len(filled_elements) > 0:
                # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ä½ç½®
                positions = filled_elements[['row', 'col']].drop_duplicates()
                if len(positions) < len(filled_elements):
                    status['has_duplicate_positions'] = True
                    status['constraint_violations'].append("å­˜åœ¨å…ƒç´ ä½ç½®å†²çª")
                
                # æ£€æŸ¥ä½ç½®å€¼æ˜¯å¦åˆç†
                if (filled_elements['row'] < 1).any() or (filled_elements['col'] < 1).any():
                    status['has_invalid_positions'] = True
                    status['constraint_violations'].append("å­˜åœ¨æ— æ•ˆä½ç½®å€¼ï¼ˆå°äº1ï¼‰")
        
        return status
    
    def copy(self):
        """
        æ·±åº¦å¤åˆ¶å½“å‰è¡¨æ ¼çŠ¶æ€
        
        è¿”å›:
            TableState: æ–°çš„è¡¨æ ¼çŠ¶æ€å¯¹è±¡
        """
        new_table = TableState(self.elem_df.copy(), self.test_df)
        new_table.elem_attrs = self.elem_attrs.copy()
        return new_table

    def get_complete_state(self):
        state_str = 'All elements:\n'
        state_str += self.state_as_long_str() + '\n\n'
        for attr in self.elem_attrs:
            state_str += f'{attr} of current elements in virtual periodic table\n'
            state_str += self.att_table(attr) + '\n\n'
        return state_str

def _infer_function_name_from_code(code: str, preferred_names=None):
    """Try to infer a function name from code. Prefer names in preferred_names if provided."""
    try:
        import ast
        tree = ast.parse(code)
        func_names = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        if preferred_names:
            for name in preferred_names:
                if name in func_names:
                    return name
        if func_names:
            # pick the last defined function as default
            return func_names[-1]
    except Exception:
        pass
    # Regex fallback
    try:
        import re
        m = re.search(r"def\s+([A-Za-z_][A-Za-z0-9_]*)\s*\(", code)
        if m:
            name = m.group(1)
            if not preferred_names or name in preferred_names:
                return name
    except Exception:
        pass
    if preferred_names:
        # As a last resort, return the first preferred name
        return preferred_names[0]
    return None

def _resolve_func_name(code: str, func_name: str, preferred_names=None) -> str:
    """Resolve a usable function name. If missing/None, infer from code, honoring preferred names."""
    if isinstance(func_name, str) and func_name and func_name.strip().lower() != 'none':
        return func_name.strip()
    inferred = _infer_function_name_from_code(code, preferred_names)
    if inferred and isinstance(inferred, str):
        return inferred
    # Avoid passing literal 'None' into executor
    return (preferred_names[0] if preferred_names else '')

def hypo_gen_and_eval(table, agents, history, decision, logger, max_retries=4, attr_name_map=None, mode='test', current_match_rate=0.0):
    # æ¸…é™¤æ–°å…ƒç´ 
    table.clear_new_elems()
    # è·å–å½“å‰çŠ¶æ€
    state = table.get_complete_state()

    # abduction
    print(logger.new_part('Abduction Process'))
    ab_agent = agents['ab_agent']
    if decision == 'C':
        
        attribute_result = ab_agent.select_main_attribute(table,state, history)
        main_attr = attribute_result['attribute']
        ascending = attribute_result['ascending'].lower() == 'true'
        table.sort_table(main_attr, ascending=ascending)
        sorted_state = table.get_complete_state()

        print('Reasoning:')
        print_and_enter(attribute_result["reasoning"])
        print('Main attribute:')
        print_and_enter(main_attr + f' ascending={ascending}')
    else:

        main_attr = history.records[-1]['attribute']
        ascending = history.records[-1]['ascending']
        table.sort_table(main_attr, ascending=ascending)
        sorted_state = table.get_complete_state()



    hypothesis_result = ab_agent.generate_hypothesis(table,sorted_state, main_attr, history)


    hypothesis = hypothesis_result['hypothesis']
    code = hypothesis_result['code']
    func_name = _resolve_func_name(
        code,
        hypothesis_result.get('func_name'),
        preferred_names=['fill_table', 'solve', 'main', 'apply_hypothesis']
    )
    
    print(f"ğŸ§ª æ‰§è¡Œæ¨¡å¼: {'æµ‹è¯•æ¨¡å¼(only_code_execution)' if mode=='test' else 'æ¨ç†æ¨¡å¼(enhanced_code_execution)'}")
    # æ‰§è¡Œç”Ÿæˆçš„å‰å‘ä»£ç 
    if mode == 'test':
        execution_result = only_code_execution(
            code, func_name, table.elem_df.copy(), max_retries
        )
    else:
        execution_result = enhanced_code_execution(
            code, func_name, table.elem_df.copy(), hypothesis, max_retries
        )
    
    if not execution_result['success']:
        print_and_enter(f"ä»£ç æ‰§è¡Œå¤±è´¥: {execution_result['error']}")
        raise Exception(f"æ— æ³•ç”Ÿæˆå¯æ‰§è¡Œçš„ä»£ç : {execution_result['error']}")
    
    actions = execution_result['result']
    
    # æ˜¾ç¤ºæ‰§è¡Œè¯¦æƒ…
    print_and_enter(f"âœ… ä»£ç æ‰§è¡ŒæˆåŠŸï¼Œå°è¯•æ¬¡æ•°: {execution_result['attempts']}")
    if execution_result.get('test_result'):
        print_and_enter(f"ğŸ“‹ å•å…ƒæµ‹è¯•ç»“æœ: {execution_result['test_result']['overall_passed']}")

    
    success = True

    print('Reasoning:')
    print_and_enter(hypothesis_result["reasoning"])
    print('Hypothesis:')
    print_and_enter(hypothesis)
    print('Code:')
    print_and_enter(code)

    # ä»…å½“actionséç©ºæ—¶æ‰å¡«å……ä½ç½®ï¼Œé¿å…å°†æœ‰æ•ˆåæ ‡è¦†ç›–ä¸ºNaN
    if actions and len(actions) > 0:
        table.fill_elem_posi(actions)
    state = table.get_complete_state()
    print(state)

    # deduction
    print(logger.new_part('Deduction Process'))
    de_agent = agents['de_agent']

    pred_result = de_agent.predict_elements(state, hypothesis, code, history,n=10)
    new_elems_posi = pred_result['new_elems_posi']
    inverse_code = pred_result['inverse_code']
    func_name = _resolve_func_name(
        inverse_code,
        pred_result.get('func_name'),
        preferred_names=['inverse_func', 'inverse', 'predict_attributes']
    )
    
    # æ‰§è¡Œé€†å‘ä»£ç 
    if mode == 'test':
        execution_result = only_code_execution(
            inverse_code, func_name, (new_elems_posi, table.elem_df.copy()), max_retries
        )
    else:
        execution_result = enhanced_code_execution(
            inverse_code, func_name, (new_elems_posi, table.elem_df.copy()), 
            f"æ ¹æ®å…ƒç´ ä½ç½®é¢„æµ‹å…ƒç´ å±æ€§ï¼Œç¬¦åˆå‡è®¾: {hypothesis}", max_retries
        )
    
    if not execution_result['success']:
        print(f"åå‘ä»£ç æ‰§è¡Œå¤±è´¥: {execution_result['error']}")
        raise Exception(f"æ— æ³•ç”Ÿæˆå¯æ‰§è¡Œçš„åå‘ä»£ç : {execution_result['error']}")
    
    new_elems = execution_result['result']
    
    # æ˜¾ç¤ºæ‰§è¡Œè¯¦æƒ…
    print(f"âœ… åå‘ä»£ç æ‰§è¡ŒæˆåŠŸï¼Œå°è¯•æ¬¡æ•°: {execution_result['attempts']}")
    if execution_result.get('test_result'):
        print(f"ğŸ“‹ å•å…ƒæµ‹è¯•ç»“æœ: {execution_result['test_result']['overall_passed']}")

    
    success = True

    print('Reasoning:')
    print_and_enter(pred_result["reasoning"])
    print('New elem position:')
    print(new_elems_posi)
    print('Inverse code:')
    print_and_enter(inverse_code)

    table.append_new_elem(new_elems)
    state = table.get_complete_state()
    print(state)

    # induction
    print(logger.new_part('Induction Process'))
    in_agent = agents['in_agent']

    new_elem_df = table.elem_df[table.elem_df.index.str.contains("NewElem")]
    matched_df = table.find_matched_elements(new_elem_df, table.test_df)
    
    print('ğŸ“‹ åŒ¹é…ç»“æœè¯¦æƒ…:')
    if not matched_df.empty:
        print('âœ… æ‰¾åˆ°åŒ¹é…çš„å…ƒç´ :')
        print(table.show_matching_details(matched_df, table.test_df))
    else:
        print('âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„å…ƒç´ ')
    
    # è®¡ç®—æ–°çš„åŒ¹é…ç‡ï¼ˆåŸºäºåˆ†æ•°ï¼‰
    if not matched_df.empty:
        # è®¡ç®—å¹³å‡åŒ¹é…åˆ†æ•°
        avg_match_score = matched_df['match_score'].mean()
        # è¯„åˆ†è§„åˆ™æ›´æ–°ï¼šå®Œå…¨åŒ¹é…ä»¥â€œæµ‹è¯•é›†å…ƒç´ â€ä¸ºå‡†ï¼Œå¤šä¸ªæ–°å…ƒç´ åŒ¹é…åŒä¸€ä¸ªæµ‹è¯•å…ƒç´ åªè®¡ä¸€æ¬¡
        threshold = 0.7
        perfect_matched_df = matched_df[matched_df['match_score'] >= threshold]
        unique_matched_targets = set(perfect_matched_df['matched_with'])
        perfect_matches = len(unique_matched_targets)

        # ä»¥æµ‹è¯•é›†å…ƒç´ æ€»æ•°ä½œä¸ºåŸºå‡†ï¼Œè¡¡é‡â€œè¦†ç›–ç‡â€
        total_test_elems = table.test_df.shape[0] if hasattr(table, 'test_df') else 1
        match_rate = perfect_matches / total_test_elems if total_test_elems > 0 else 0.0
        
        # é‡å¤åŒ¹é…ç»Ÿè®¡ï¼šåŒä¸€æµ‹è¯•å…ƒç´ è¢«å¤šä¸ªæ–°å…ƒç´ å‘½ä¸­
        target_counts = perfect_matched_df['matched_with'].value_counts()
        duplicate_targets = target_counts[target_counts > 1]
        num_duplicate_groups = duplicate_targets.shape[0]
        num_redundant_preds = int(duplicate_targets.sum() - num_duplicate_groups) if num_duplicate_groups > 0 else 0
        
        print(f'\nğŸ“Š åŒ¹é…ç»Ÿè®¡:')
        print(f'   å¹³å‡åŒ¹é…åˆ†æ•°: {avg_match_score:.3f}')
        print(f'   å®Œå…¨åŒ¹é…ï¼ˆæŒ‰æµ‹è¯•é›†å»é‡ï¼‰: {perfect_matches}')
        print(f'   æµ‹è¯•é›†å…ƒç´ æ€»æ•°: {total_test_elems}')
        print(f'   è¦†ç›–ç‡(å”¯ä¸€ç›®æ ‡): {match_rate:.3f}')
        if num_duplicate_groups > 0:
            print(f'   âš ï¸ é‡å¤åŒ¹é…ç›®æ ‡æ•°: {num_duplicate_groups}ï¼Œå†—ä½™æ–°å…ƒç´ æ•°: {num_redundant_preds}')
        
        # çº¦æŸæ£€æŸ¥å’Œæ€§èƒ½è¯„ä¼°
        print(f'\nğŸ” çº¦æŸæ£€æŸ¥å’Œæ€§èƒ½è¯„ä¼°:')
        constraint_violations = []
        
       
        
        # æ£€æŸ¥åŒ¹é…ç‡æ˜¯å¦ä¸‹é™ï¼ˆä¸å†å²è®°å½•æ¯”è¾ƒï¼‰
        if len(history.records) > 1:  # è‡³å°‘æœ‰ä¸€ä¸ªå†å²è®°å½•
            # è¿‡æ»¤æ‰éå­—å…¸çš„è®°å½•
            valid_records = [record for record in history.records if isinstance(record, dict)]
            if len(valid_records) > 1:
                # è·å–ä¸Šä¸€æ¬¡çš„åŒ¹é…ç‡
                last_match_rate = valid_records[-1].get('match_rate', 0.0)
                if match_rate < last_match_rate:
                    decline = last_match_rate - match_rate
                    constraint_violations.append(f"åŒ¹é…ç‡ä¸‹é™: {match_rate:.3f} < {last_match_rate:.3f} (ä¸‹é™ {decline:.3f})")
                    print(f"ğŸ“‰ åŒ¹é…ç‡ä¸‹é™æ£€æµ‹: å½“å‰ {match_rate:.3f} vs ä¸Šæ¬¡ {last_match_rate:.3f}")
        
        # é’ˆå¯¹é‡å¤åŒ¹é…çš„çº¦æŸæé†’
        if num_duplicate_groups > 0:
            constraint_violations.append(
                f"å‘ç°å¤šä¸ªæ–°å…ƒç´ åŒ¹é…åˆ°åŒä¸€æµ‹è¯•å…ƒç´ ï¼š{num_duplicate_groups} ä¸ªç›®æ ‡å­˜åœ¨é‡å¤åŒ¹é…ï¼›é‡å¤å°†æŒ‰ä¸€æ¬¡è®¡åˆ†ï¼Œå†—ä½™é¢„æµ‹åº”å‡å°‘"
            )
        
        # ä½¿ç”¨æ–°çš„çº¦æŸæ£€æŸ¥æ–¹æ³•
        constraint_status = table.get_constraint_status()
        if constraint_status['constraint_violations']:
            constraint_violations.extend(constraint_status['constraint_violations'])
        
        if constraint_violations:
            print(f"âŒ å‘ç°çº¦æŸè¿å:")
            for violation in constraint_violations:
                print(f"   - {violation}")
           
        else:
            print("âœ… çº¦æŸæ£€æŸ¥é€šè¿‡ï¼Œæ€§èƒ½è¡¨ç°è‰¯å¥½")
            
    else:
        avg_match_score = 0.0
        match_rate = 0.0
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„å…ƒç´ ")
        print("ğŸ’¡ å»ºè®®é€‰æ‹©Ré€‰é¡¹è¿›è¡Œå›æ»š")
    
    matched_elem_str = table.state_as_long_str(matched_df)
    
    # ä½¿ç”¨ä¼ å…¥çš„current_match_rateä½œä¸ºprevious_match_rate
    eval_result = in_agent.evaluate_hypothesis(
        state, hypothesis, matched_elem_str, match_rate, avg_match_score, current_match_rate
    )
    evaluation = eval_result['evaluation']
    decision = eval_result['decision']

    print('Reasoning:')
    print_and_enter(eval_result["reasoning"])
    print('evaluation:')
    print_and_enter(evaluation)
    print('decision:')
    print_and_enter(in_agent.options[decision])

    history.update_records(hypothesis, evaluation, match_rate, main_attr, ascending, decision)

    return table, history, decision, matched_df

if __name__ == '__main__':
    
    data_path = r'/Users/yuliachu/Documents/GitHub/SciPuzzleLLM_cy/cy_track/data'# make table data and object
    #table_file = join(data_path, 'PeriodicTable.csv')
    #table_file = join(data_path, 'PeriodicTable_mjlf33.csv')
    #table_file = join(data_path, 'PeriodicTable_complete.csv')
    #table_file = join(data_path, 'PeriodicTableBaseline.csv')
    table_file = join(data_path, 'PeriodicTable_puzzle2.csv')
    if not os.path.exists(table_file):
        df = generate_table()
        df.to_csv(table_file, index=False)
    else:
        df = pd.read_csv(table_file, index_col=None)

    train_df, test_df, attr_name_map = mask_table(df)
    train_df.to_csv(join(data_path, 'train_df.csv'), index=False)
    train_df = train_df.set_index('Element', drop=True)
    test_df.to_csv(join(data_path, 'test_df.csv'), index=False)
    test_df = test_df.set_index('Element', drop=True)

    train_df = pd.read_csv(join(data_path, 'train_df.csv'), index_col='Element')
    test_df = pd.read_csv(join(data_path, 'test_df.csv'), index_col='Element')

    table = TableState(train_df, test_df)
    
    agents = {
        'ab_agent': AbductionAgent(),
        'de_agent': DeductionAgent(),
        'in_agent': InductionAgent()
    }

    history = History()
    
    # Optionally load previous records from a specific log
    #history.load_records_from_log(join(data_path, 'logs', '2025-07-04-13-17-03'), iteration=1)
    logger = Logger(join(data_path, 'logs', 'new_experiment'))
    
    #logger = Logger(join(data_path, 'logs'))
    max_iter = 7
    max_retries = 1
    decision = 'C'
    matched_df = None
    
    # ç»Ÿè®¡ä¿¡æ¯
    successful_iterations = 0
    failed_iterations = 0
    
    # ä¿å­˜åˆå§‹çŠ¶æ€ï¼ˆä½œä¸ºå›æ»šçš„åŸºå‡†ç‚¹ï¼‰
    initial_table_state = table
    initial_history_state = history
    initial_decision_state = decision
    initial_matched_df = matched_df
    
    # å–æ¶ˆ last_successful_* ç­–ç•¥ï¼Œç»Ÿä¸€ä¾èµ–å›æ»šåˆ° pre_induction_* çŠ¶æ€
    
    try:
        for i in range(max_iter):

            print(logger.new_part(f'Iteration {i+1}'))

            try:
                # åœ¨inductioné˜¶æ®µä¹‹å‰ä¿å­˜å½“å‰çŠ¶æ€ï¼ˆæ·±åº¦å¤åˆ¶ï¼‰
                pre_induction_table = table.copy() if hasattr(table, 'copy') else table
                pre_induction_history = history.copy() if hasattr(history, 'copy') else history
                pre_induction_decision = decision
                pre_induction_matched_df = matched_df
                
                # ä¿å­˜å½“å‰çš„åŒ¹é…ç‡ï¼Œç”¨äºä¸‹æ¬¡è¿­ä»£æ¯”è¾ƒ
                current_match_rate = 0.0
                if len(history.records) > 0:
                    last_record = history.records[-1]
                    if isinstance(last_record, dict) and 'match_rate' in last_record:
                        current_match_rate = last_record['match_rate']
                
                print(f"ğŸ“‹ ä¿å­˜è¿­ä»£å‰çŠ¶æ€:")
                print(f"   è¡¨æ ¼å…ƒç´ æ•°é‡: {len(table.elem_df) if hasattr(table, 'elem_df') else 'N/A'}")
                print(f"   å†å²è®°å½•æ•°é‡: {len(history.records) if hasattr(history, 'records') else 'N/A'}")
                print(f"   å½“å‰å†³ç­–: {decision}")
                print(f"   å½“å‰åŒ¹é…ç‡: {current_match_rate:.3f}")
                
                table, history, decision, matched_df = hypo_gen_and_eval(
                    table, agents, history, decision, logger, max_retries, attr_name_map, mode='work', current_match_rate=current_match_rate)
                
                print(f"âœ… ç¬¬ {i+1} è½®è¿­ä»£æˆåŠŸå®Œæˆ")
                successful_iterations += 1
                
                if decision == 'P':
                    print('Stop as the hypothesis is accepted!')
                    break
                elif decision == 'R':
                    print('ğŸ”„ Decision R: Rolling back to previous version due to constraint violation or poor performance')
                    # å›æ»šåˆ°inductioné˜¶æ®µä¹‹å‰çš„çŠ¶æ€
                    if pre_induction_table is not None:
                        table = pre_induction_table
                        history = pre_induction_history
                        decision = pre_induction_decision
                        matched_df = pre_induction_matched_df # æ¸…ç©ºåŒ¹é…ç»“æœ
                        
                        # åˆ é™¤ä¸Šä¸€æ¡å†å²è®°å½•ï¼ˆå½“å‰å¤±è´¥çš„è¿­ä»£è®°å½•ï¼‰
                        if len(history.records) > 1:  # ç¡®ä¿è‡³å°‘æœ‰ä¸€æ¡è®°å½•å¯ä»¥åˆ é™¤
                            removed_record = history.records.pop()
                            print(f"ğŸ—‘ï¸ å·²åˆ é™¤éœ€è¦å›æ»šçš„è¿­ä»£è®°å½•: {removed_record.get('hypothesis', 'N/A')[:50]}...")
                        
                        print("âœ… å·²å›æ»šåˆ°inductioné˜¶æ®µä¹‹å‰çš„çŠ¶æ€")
                        print(f"ğŸ”„ å›æ»šåçš„å†³ç­–: {decision}")
                        
                        # éªŒè¯å›æ»šæ˜¯å¦æˆåŠŸ
                        print(f"ğŸ“‹ å›æ»šåçŠ¶æ€éªŒè¯:")
                        print(f"   è¡¨æ ¼å…ƒç´ æ•°é‡: {len(table.elem_df) if hasattr(table, 'elem_df') else 'N/A'}")
                        print(f"   å†å²è®°å½•æ•°é‡: {len(history.records) if hasattr(history, 'records') else 'N/A'}")
                        print(f"   å†³ç­–çŠ¶æ€: {decision}")
                        
                        # ç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£ï¼Œè·³è¿‡å½“å‰å¤±è´¥çš„ç‰ˆæœ¬
                        continue
                    else:
                        print("âš ï¸  æ— æ³•å›æ»šï¼Œä½¿ç”¨åˆå§‹çŠ¶æ€")
                        table = initial_table_state
                        history = initial_history_state
                        decision = initial_decision_state
                        matched_df = None
                        
                        print(f"ğŸ“‹ ä½¿ç”¨åˆå§‹çŠ¶æ€:")
                        print(f"   è¡¨æ ¼å…ƒç´ æ•°é‡: {len(table.elem_df) if hasattr(table, 'elem_df') else 'N/A'}")
                        print(f"   å†å²è®°å½•æ•°é‡: {len(history.records) if hasattr(history, 'records') else 'N/A'}")
                        print(f"   å†³ç­–çŠ¶æ€: {decision}")
                        
                        continue
                    
            except Exception as e:
                print(f"âŒ ç¬¬ {i+1} è½®è¿­ä»£å‡ºç°é”™è¯¯: {e}")
                failed_iterations += 1
                print("ğŸ”„ è·³è¿‡æœ¬æ¬¡è¿­ä»£ï¼Œä½¿ç”¨ä¸Šæ¬¡è¿­ä»£çš„æ­£å¸¸ç»“æœç»§ç»­...")
                
                # å‡ºé”™æ—¶å›æ»šåˆ°æœ¬è½® induction å‰ä¿å­˜çš„çŠ¶æ€ï¼ˆæ³¨æ„ï¼šå¼‚å¸¸æƒ…å†µä¸‹è®°å½•æœªä¿å­˜ï¼Œä¸éœ€è¦åˆ é™¤ï¼‰
                if 'pre_induction_table' in locals() and pre_induction_table is not None:
                    table = pre_induction_table
                    history = pre_induction_history
                    decision = pre_induction_decision
                    matched_df = pre_induction_matched_df
                    
                    # å¼‚å¸¸æƒ…å†µä¸‹è®°å½•æœªä¿å­˜ï¼Œä¸éœ€è¦åˆ é™¤å†å²è®°å½•
                    print("âœ… å·²å›æ»šåˆ°inductioné˜¶æ®µä¹‹å‰çš„çŠ¶æ€ï¼ˆé”™è¯¯å¤„ç†ï¼‰")
                    print("ğŸ“ æ³¨æ„ï¼šå¼‚å¸¸æƒ…å†µä¸‹è®°å½•æœªä¿å­˜ï¼Œæ— éœ€åˆ é™¤å†å²è®°å½•")
                else:
                    print("âš ï¸  æ— æ³•å›æ»šï¼Œä½¿ç”¨åˆå§‹çŠ¶æ€ï¼ˆé”™è¯¯å¤„ç†ï¼‰")
                    table = initial_table_state
                    history = initial_history_state
                    decision = initial_decision_state
                    matched_df = initial_matched_df
                
                # è®°å½•é”™è¯¯åˆ°æ—¥å¿—
                print(f"âŒ ç¬¬ {i+1} è½®è¿­ä»£é”™è¯¯è®°å½•: {e}")
                continue

        if i == max_iter - 1:
            print(f'Stop as the max iteration {i+1} is reached!')

        # æ˜¾ç¤ºè¿­ä»£ç»Ÿè®¡ä¿¡æ¯
        print(f'\nğŸ“Š è¿­ä»£ç»Ÿè®¡ä¿¡æ¯:')
        print(f'   æˆåŠŸè¿­ä»£æ¬¡æ•°: {successful_iterations}')
        print(f'   å¤±è´¥è¿­ä»£æ¬¡æ•°: {failed_iterations}')
        print(f'   æ€»è¿­ä»£æ¬¡æ•°: {max_iter}')
        print(f'   æˆåŠŸç‡: {successful_iterations/max_iter*100:.1f}%')

        print('\nhistory:')
        print(history.show_records())

        """  # é€‰æ‹©æœ€ä¼˜å‡è®¾å¹¶è¿›è¡Œæç‚¼æ€»ç»“
        print("\n" + "="*50)
        print("é€‰æ‹©æœ€ä¼˜å‡è®¾å¹¶è¿›è¡Œæç‚¼æ€»ç»“")
        print("="*50) 
        
        record_agent = RecordAgent()
        optimal_hypothesis_summary = record_agent.merge_records(history.records)
        
        if optimal_hypothesis_summary:
            try:
                # è§£æJSONå“åº”ï¼ˆå¥å£®åŒ–å¤„ç†ï¼‰
                summary_data = None
                import json, re
                if isinstance(optimal_hypothesis_summary, dict):
                    summary_data = optimal_hypothesis_summary
                else:
                    raw_text = str(optimal_hypothesis_summary).strip()
                    if raw_text:
                        try:
                            summary_data = json.loads(raw_text)
                        except Exception:
                            # å°è¯•æå–ç¬¬ä¸€ä¸ªJSONå¯¹è±¡
                            m = re.search(r"\{[\s\S]*\}", raw_text)
                            if m:
                                try:
                                    summary_data = json.loads(m.group(0))
                                except Exception:
                                    summary_data = None
                
                if not isinstance(summary_data, dict):
                    print("âš ï¸ æœ€ä¼˜å‡è®¾è¿”å›æ ¼å¼éJSONï¼Œè·³è¿‡åº”ç”¨æœ€ä¼˜å‡è®¾ã€‚åŸå§‹å†…å®¹é¢„è§ˆï¼š")
                    print(str(optimal_hypothesis_summary)[:300])
                    summary_data = None
                
                if summary_data is not None:
                    print("âœ… æœ€ä¼˜å‡è®¾æ€»ç»“:")
                    print(f"   å‡è®¾: {summary_data.get('hypothesis', 'N/A')}")
                    print(f"   ä¸»å±æ€§: {summary_data.get('attribute', 'N/A')}")
                    print(f"   å‡åºæ’åˆ—: {summary_data.get('ascending', 'N/A')}")
                    print(f"   åŒ¹é…ç‡: {summary_data.get('match_rate', 'N/A')}")
                
                    # å°†æœ€ä¼˜å‡è®¾å†™å…¥logç›®å½•ï¼Œä¾¿äºå¤ç°å’Œç»˜å›¾
                    try:
                        summary_path = join(logger.log_folder, 'optimal_hypothesis.json')
                        with open(summary_path, 'w', encoding='utf-8') as f:
                            json.dump(summary_data, f, ensure_ascii=False, indent=2)
                        print(f"ğŸ“ å·²ä¿å­˜æœ€ä¼˜å‡è®¾åˆ°: {summary_path}")
                    except Exception as e:
                        print(f"âš ï¸ æœ€ä¼˜å‡è®¾ä¿å­˜å¤±è´¥: {e}")
                
                    # å°†æœ€ä¼˜å‡è®¾åº”ç”¨åˆ°æœ€ç»ˆè¡¨æ ¼
                    if 'attribute' in summary_data and summary_data['attribute']:
                        main_attr = summary_data['attribute']
                        ascending = summary_data.get('ascending', True)
                        if isinstance(ascending, str):
                            ascending = ascending.lower() == 'true'
                        
                        print(f"\nğŸ”„ åº”ç”¨æœ€ä¼˜å‡è®¾åˆ°æœ€ç»ˆè¡¨æ ¼:")
                        print(f"   ä¸»å±æ€§: {main_attr}")
                        print(f"   å‡åºæ’åˆ—: {ascending}")
                        
                        # é‡æ–°æ’åºè¡¨æ ¼
                        table.sort_table(main_attr, ascending=ascending)
                        final_state = table.get_complete_state()
                        print("âœ… è¡¨æ ¼å·²æŒ‰æœ€ä¼˜å‡è®¾é‡æ–°æ’åº")
                    
            except Exception as e:
                print(f"âš ï¸  è§£ææœ€ä¼˜å‡è®¾æ€»ç»“æ—¶å‡ºé”™: {e}")
                print("ä½¿ç”¨å½“å‰è¡¨æ ¼çŠ¶æ€è¿›è¡Œå¯è§†åŒ–")
        else:
            print("âš ï¸  æ— æ³•è·å–æœ€ä¼˜å‡è®¾æ€»ç»“ï¼Œä½¿ç”¨å½“å‰è¡¨æ ¼çŠ¶æ€") """

        final_df = table.elem_df.copy()
        # æ–°å¢ KnownAndMatched åˆ—ï¼Œæ ¹æ®åŒ¹é…åˆ†æ•°åˆ†ç±»
        
        if matched_df is not None and not matched_df.empty:
            # åˆ›å»ºåŒ¹é…åˆ†æ•°æ˜ å°„å’Œé‡å¤åŒ¹é…å¤„ç†
            threshold = 0.7
            match_score_map = {}
            target_matches = {}  # è®°å½•æ¯ä¸ªæµ‹è¯•ç›®æ ‡çš„åŒ¹é…æƒ…å†µ
            
            # ç»Ÿè®¡æ¯ä¸ªæµ‹è¯•ç›®æ ‡çš„åŒ¹é…æƒ…å†µ
            for idx, row in matched_df.iterrows():
                try:
                    score = float(row.get('match_score', 0.0))
                    target = row.get('matched_with', None)
                    if score >= threshold and target:
                        match_score_map[idx] = score
                        if target not in target_matches:
                            target_matches[target] = []
                        target_matches[target].append((idx, score))
                except Exception:
                    continue
            
            # å¤„ç†é‡å¤åŒ¹é…ï¼šä¸ºæ¯ä¸ªæµ‹è¯•ç›®æ ‡åªä¿ç•™ä¸€ä¸ªæœ€ä½³åŒ¹é…ï¼Œè®°å½•åŒ¹é…æ•°
            final_match_info = {}  # è®°å½•æœ€ç»ˆä¿ç•™çš„åŒ¹é…ä¿¡æ¯
            for target, matches in target_matches.items():
                if len(matches) > 1:
                    # æŒ‰åˆ†æ•°æ’åºï¼Œä¿ç•™æœ€é«˜åˆ†çš„
                    matches.sort(key=lambda x: x[1], reverse=True)
                    best_match = matches[0][0]
                    match_count = len(matches)
                    final_match_info[best_match] = {
                        'score': matches[0][1],
                        'match_count': match_count,
                        'target': target
                    }
                    # ç§»é™¤å…¶ä»–é‡å¤åŒ¹é…
                    for idx, _ in matches[1:]:
                        if idx in match_score_map:
                            del match_score_map[idx]
                else:
                    # åªæœ‰ä¸€ä¸ªåŒ¹é…
                    idx, score = matches[0]
                    final_match_info[idx] = {
                        'score': score,
                        'match_count': 1,
                        'target': target
                    }
            
            # åˆ›å»º KnownAndMatched åˆ—
            final_df['KnownAndMatched'] = final_df.index.map(
                lambda elem: 'Known' if not str(elem).startswith('NewElem') 
                else 'KnownAndMatched' if (elem in match_score_map and match_score_map[elem] >= threshold)
                else 'New&Unmatched'
            )
            
            # æ·»åŠ åŒ¹é…æ•°ä¿¡æ¯åˆ—
            final_df['MatchCount'] = final_df.index.map(
                lambda elem: final_match_info.get(elem, {}).get('match_count', 1) if elem in final_match_info else 1
            )

            # è°ƒè¯•è¾“å‡ºï¼šå“ªäº›æ–°å…ƒç´ åŒ¹é…ä¸Šäº†ï¼ˆ>=0.7ï¼‰ï¼Œå“ªäº›æ²¡æœ‰
            matched_items = []
            for idx, info in final_match_info.items():
                matched_items.append((idx, info['target'], info['score'], info['match_count']))

            unmatched_items = [
                elem for elem in final_df.index
                if str(elem).startswith('NewElem') and not (
                    elem in match_score_map and match_score_map[elem] >= threshold
                )
            ]

            print('\nğŸ§© å¯è§†åŒ–åˆ†ç±»æ£€æŸ¥:')
            print(f'   è¾¾åˆ°é˜ˆå€¼(>= {threshold})çš„æ–°å…ƒç´ æ•°: {len(matched_items)}')
            if len(matched_items) > 0:
                preview = '\n'.join([f"      {e} -> {t} (score={s:.2f}, count={c})" for e,t,s,c in matched_items[:10]])
                print('   ç¤ºä¾‹(å‰10):')
                print(preview)
            print(f'   æœªè¾¾åˆ°é˜ˆå€¼çš„æ–°å…ƒç´ æ•°: {len(unmatched_items)}')
            if len(unmatched_items) > 0:
                preview2 = '\n'.join([f"      {e}" for e in unmatched_items[:10]])
                print('   ç¤ºä¾‹(å‰10):')
                print(preview2)
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…æ•°æ®ï¼Œæ‰€æœ‰æ–°å…ƒç´ éƒ½æ ‡è®°ä¸ºæœªåŒ¹é…
            final_df['KnownAndMatched'] = final_df.index.map(
                lambda elem: 'Known' if not str(elem).startswith('NewElem') 
                else 'New&Unmatched'
            )
            final_df['MatchCount'] = 1  # é»˜è®¤åŒ¹é…æ•°ä¸º1

        logger.log_table_as_csv(final_df)
        
        # ä¸ºMatchCountå¯è§†åŒ–å‡†å¤‡åŒ¹é…æ•°æ®
        if matched_df is not None and not matched_df.empty:
            # åˆ›å»ºåŒ¹é…æ•°æ®çš„å‰¯æœ¬ï¼Œç”¨äºå¯è§†åŒ–
            viz_matched_df = matched_df.copy()
        else:
            viz_matched_df = None
        
        logger.log_table_as_img(final_df, matched_df=viz_matched_df)
        
        # è§£ælogæ–‡ä»¶ï¼Œç”Ÿæˆå¤„ç†åçš„ç»“æœ
        print("\n" + "="*50)
        print("å¼€å§‹è§£ælogæ–‡ä»¶...")
        parsed_output_path = join(logger.log_folder, 'parsed_results.txt')
        parse_log_file(logger.log_folder, parsed_output_path)
        print(f"åŸå§‹logæ–‡ä»¶: {join(logger.log_folder, 'log.txt')}")
        print(f"è§£æç»“æœæ–‡ä»¶: {parsed_output_path}")
        print("="*50)
        
    except Exception as e:
        print(f"Error: {e}")
    finally:
        logger.close()
