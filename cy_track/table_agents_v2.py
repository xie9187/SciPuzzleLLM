import openai
import time
import re
import json
import os
import getpass
import random

class Agent(object):

    def __init__(self):
        super(Agent, self).__init__()

        #self.key = "sk-PumVcl4i7eUKuTl75c47E62774B1487b8cDb056b1d92D201" #æˆ‘çš„key
        #self.url = "https://api.bltcy.ai/v1"
        #self.model = 'claude-opus-4-1-20250805-thinking' # 'deepseek-r1', 'deepseek-V3', 'o1', 'GPT-4o'


        self.key = "sk-v0LHdPEABytbnbbGvyy2WR9QLemde9EdYu52dzyiwrYg563L" #æˆ‘çš„key
        self.url = "https://happyapi.org/v1"
        self.model = 'claude-3-7-sonnet-20250219-thinking' # 'deepseek-r1', 'deepseek-v3', 'o1', 'gpt-4o'
         # é‡è¯•é…ç½®
        self.max_retries = 5
        self.base_delay = 5
        self.max_delay = 60
        self.timeout = 150

    def _exponential_backoff(self, attempt: int) -> float:
        """æŒ‡æ•°é€€é¿ç®—æ³•"""
        delay = min(self.base_delay * (2 ** attempt) + random.uniform(0, 1), self.max_delay)
        return delay

    def _is_retryable_error(self, error) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•"""
        retryable_errors = [
            '502 Bad Gateway',
            '503 Service Unavailable', 
            '504 Gateway Timeout',
            'ConnectionError',
            'TimeoutError',
            'InternalServerError',
            'new_api_error'
        ]
        
        error_str = str(error).lower()
        return any(retryable in error_str for retryable in retryable_errors)

    def get_LLM_response(self, prompt_msgs):    
        """ # Set API key and base URL
        client = openai.OpenAI(api_key=self.key, base_url=self.url)
        start_time = time.time()
        response = client.chat.completions.create(
            model=self.model,
            messages=prompt_msgs
        )
        response_time = time.time() - start_time
        print(f'Got response from {self.model} in {response_time:.1f} sec.')
        answer = response.choices[0].message.content.strip()
        return answer   """
        """è·å–LLMå“åº”ï¼Œå…·æœ‰é‡è¯•å’Œé”™è¯¯å¤„ç†"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                # è®¾ç½®APIå®¢æˆ·ç«¯
                client = openai.OpenAI(
                    api_key=self.key, 
                    base_url=self.url,
                    timeout=self.timeout
                )
                
                start_time = time.time()
                
                # å‘é€è¯·æ±‚
                response = client.chat.completions.create(
                    model=self.model,
                    messages=prompt_msgs
                )
                
                response_time = time.time() - start_time
                print(f'âœ… ä» {self.model} è·å¾—å“åº”ï¼Œç”¨æ—¶ {response_time:.1f} ç§’')
                
                answer = response.choices[0].message.content.strip()
                return answer
                
            except Exception as e:
                last_error = e
                error_msg = str(e)
                
                print(f'âŒ å°è¯• {attempt + 1}/{self.max_retries} å¤±è´¥: {error_msg}')
                
                # æ£€æŸ¥æ˜¯å¦å¯é‡è¯•
                if not self._is_retryable_error(e):
                    print(f'âŒ ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º: {error_msg}')
                    raise e
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼š
                if attempt < self.max_retries - 1:
                    delay = self._exponential_backoff(attempt)
                    print(f'â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...')
                    time.sleep(delay)
                else:
                    print(f'âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries})')
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise Exception(f"APIè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}")

    def get_LLM_response_with_tools(self, prompt_msgs, tools):
        """è·å–LLMå“åº”ï¼Œæ”¯æŒfunction calling"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                # è®¾ç½®APIå®¢æˆ·ç«¯
                client = openai.OpenAI(
                    api_key=self.key, 
                    base_url=self.url,
                    timeout=self.timeout
                )
                
                start_time = time.time()
                
                # å‘é€è¯·æ±‚ï¼ŒåŒ…å«å·¥å…·å®šä¹‰
                response = client.chat.completions.create(
                    model=self.model,
                    messages=prompt_msgs,
                    tools=tools,
                    tool_choice="auto"  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
                )
                
                response_time = time.time() - start_time
                print(f'âœ… ä» {self.model} è·å¾—å¸¦å·¥å…·çš„å“åº”ï¼Œç”¨æ—¶ {response_time:.1f} ç§’')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if response.choices[0].message.tool_calls:
                    tool_call = response.choices[0].message.tool_calls[0]
                    print(f'ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_call.function.name}')
                    
                    # å¦‚æœè°ƒç”¨äº†make_decisionå‡½æ•°ï¼Œç›´æ¥è¿”å›å“åº”å†…å®¹
                    if tool_call.function.name == "make_decision":
                        return response.choices[0].message.content or ""
                
                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ™®é€šå“åº”
                answer = response.choices[0].message.content.strip()
                return answer
                
            except Exception as e:
                last_error = e
                error_msg = str(e)
                
                print(f'âŒ å°è¯• {attempt + 1}/{self.max_retries} å¤±è´¥: {error_msg}')
                
                # æ£€æŸ¥æ˜¯å¦å¯é‡è¯•
                if not self._is_retryable_error(e):
                    print(f'âŒ ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º: {error_msg}')
                    raise e
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼š
                if attempt < self.max_retries - 1:
                    delay = self._exponential_backoff(attempt)
                    print(f'â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...')
                    time.sleep(delay)
                else:
                    print(f'âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries})')
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise Exception(f"APIè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}")
  

    def get_LLM_structured_response(self, prompt_msgs):
        """ client = openai.OpenAI(api_key=self.key, base_url=self.url)
        start_time = time.time()
        response = client.chat.completions.create(
            model=self.model,
            messages=prompt_msgs,
            response_format={
                "type": "json_object"
            }
        )
        response_time = time.time() - start_time
        print(f'Got response from {self.model} in {response_time:.1f} sec.')
        answer = response.choices[0].message.content.strip()
        return answer  """
        """è·å–ç»“æ„åŒ–LLMå“åº”ï¼Œå…·æœ‰é‡è¯•å’Œé”™è¯¯å¤„ç†"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                # è®¾ç½®APIå®¢æˆ·ç«¯
                client = openai.OpenAI(
                    api_key=self.key, 
                    base_url=self.url,
                    timeout=self.timeout
                )
                
                start_time = time.time()
                
                # å‘é€è¯·æ±‚
                response = client.chat.completions.create(
                    model=self.model,
                    messages=prompt_msgs,
                    response_format={
                        "type": "json_object"
                    }
                )
                
                response_time = time.time() - start_time
                print(f'âœ… ä» {self.model} è·å¾—ç»“æ„åŒ–å“åº”ï¼Œç”¨æ—¶ {response_time:.1f} ç§’')
                
                answer = response.choices[0].message.content.strip()
                return answer
                
            except Exception as e:
                last_error = e
                error_msg = str(e)
                
                print(f'âŒ å°è¯• {attempt + 1}/{self.max_retries} å¤±è´¥: {error_msg}')
                
                # æ£€æŸ¥æ˜¯å¦å¯é‡è¯•
                if not self._is_retryable_error(e):
                    print(f'âŒ ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º: {error_msg}')
                    raise e
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼š
                if attempt < self.max_retries - 1:
                    delay = self._exponential_backoff(attempt)
                    print(f'â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...')
                    time.sleep(delay)
                else:
                    print(f'âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries})')
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise Exception(f"APIè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}")
       
    
    # è§£æå“åº”å†…å®¹
    def extract_content(self, tag, text):
        pattern = re.compile(f'<{tag}>(.*?)</{tag}>', re.DOTALL)
        match = pattern.search(text)
        return match.group(1).strip() if match else None

    def reflaction(self, task, raw_response):
        pass

class AbductionAgent(Agent):

    def __init__(self):
        super(AbductionAgent, self).__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        # å®šä¹‰è§’è‰²æç¤ºè¯
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä½è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹ç ”ç©¶ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®è¡¨æ ¼çŠ¶æ€ä»¥åŠä»¥å¾€å†å²è®°å½•æå‡ºæˆ–ä¿®æ”¹è™šæ‹Ÿå…ƒç´ å‘¨æœŸæ€§è§„å¾‹å‡è®¾ã€‚
        """
        self.state_introduction = """
        1. å…ƒç´ è¡¨æ ¼ï¼ŒåŒ…æ‹¬å…ƒç´ åç§°ï¼ˆæ— ä»»ä½•å®é™…å«ä¹‰ï¼‰ã€å…ƒç´ å±æ€§ã€å…ƒç´ åœ¨è™šæ‹Ÿå‘¨æœŸè¡¨æ ¼çš„ä½ç½®ï¼ˆè‹¥å·²è¢«å¡«å…¥ï¼‰
        2. ä»¥è™šæ‹Ÿå…ƒç´ è¡¨æ ¼å½¢å¼å‘ˆç°çš„æ‰€æœ‰å·²è¢«å¡«å…¥è¡¨æ ¼å…ƒç´ å±æ€§å€¼

        """

    def select_main_attribute(self, table,state, history):

        task = """
        1. é€‰æ‹©ä¸€ä¸ªå±æ€§ä½œä¸ºå…ƒç´ æ’å¸ƒçš„ä¸»å±æ€§ï¼Œä½¿å¾—å…¶ä»–å±æ€§èƒ½å°½å¯èƒ½å‘ˆç°æŒ‰è¡Œå‘¨æœŸå˜åŒ–ã€æŒ‰åˆ—ç›¸ä¼¼æˆ–æ¸å˜çš„è§„å¾‹
        2. ä¸»å±æ€§å¿…é¡»ä¸¥æ ¼æŒ‰å‡åºæ’åˆ—ï¼Œä¸èƒ½æœ‰é‡å¤å€¼ï¼Œæ¯ä¸ªå…ƒç´ çš„ä¸»å±æ€§å€¼å¿…é¡»å”¯ä¸€
        3. å‘¨æœŸè¡¨çš„è¡Œå’Œåˆ—å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸»å±æ€§å€¼çš„å¤§å°é¡ºåºæ’åˆ—ï¼Œç¡®ä¿ä¸»å±æ€§å€¼ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹ä¸¥æ ¼é€’å¢
        4. å¦‚æœå‘ç°ä¸»å±æ€§æœ‰é‡å¤å€¼ï¼Œå¿…é¡»é‡æ–°é€‰æ‹©ä¸»å±æ€§
        """

        user_prompt = f"""
        å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€åŒ…æ‹¬ï¼š
        <state introduction>
        {self.state_introduction}
        </state introduction>

        è™šæ‹Ÿå…ƒç´ åŠå±æ€§ä¿¡æ¯ï¼š
        <elem_df>
        {table.elem_df}
        </elem_df>

        è¯·æŒ‰ç…§å¦‚ä¸‹è¦æ±‚å®Œæˆï¼š
        <task>
        {task}
        </task>

        ä»¥å¾€çš„å†å²è®°å½•ï¼ˆåŒ…å«å‡è®¾ã€è¯„ä»·ä¸é¢„æµ‹å…ƒç´ æˆåŠŸåŒ¹é…ç‡ï¼‰ä¸ºï¼š
        <history>
        {history.select_record()}
        </history>

        ä½ çš„å›å¤å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š

        <reasoning>
        æ­¤å¤„ç»™å‡ºæ¨ç†è¿‡ç¨‹ï¼Œè¦æ±‚å¯¹æ¯”æ¯ä¸ªå±æ€§ä½œä¸ºä¸»å±æ€§çš„ä¼˜åŠ£å¹¶é€‰å‡ºæœ€æœ‰åˆ©äºå½’çº³è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹çš„ä¸»å±æ€§
        </reasoning>

        <attribute>
        æ­¤å¤„ç»™å‡ºä¸»å±æ€§çš„åå­—
        </attribute>

        <ascending>
        æ­¤å¤„ç»™å‡ºæ˜¯å¦æŒ‰å‡åºæ’åˆ—ä¸»å±æ€§ï¼Œä»…å›å¤Trueæˆ–False
        </ascending>
        """

        # æ„é€ ç¬¦åˆAPIè¦æ±‚çš„messagesåˆ—è¡¨
        prompt_msgs = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        raw_response = self.get_LLM_response(prompt_msgs)

        return {
            "task": task,
            "reasoning": self.extract_content("reasoning", raw_response),
            "attribute": self.extract_content("attribute", raw_response),
            "ascending": self.extract_content("ascending", raw_response),
            "raw_response": raw_response  # ä¿ç•™åŸå§‹å“åº”ä»¥é˜²è§£æå¤±è´¥
        }

    def generate_hypothesis(self, table,state, attribute, history):    
        
        task = f"""
        1. è¯†åˆ«åœ¨ä¸»å±æ€§é¡ºåºæ’åˆ—æ—¶å…¶ä»–å±æ€§æ˜¯å¦å‘ˆç°ä»¥åŠå‘ˆç°ä½•ç§å‘¨æœŸè§„å¾‹
        2. æå‡ºæœ€å¯èƒ½çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹å‡è®¾ï¼Œä½¿å°½å¯èƒ½å¤šçš„å…ƒç´ æ»¡è¶³
        3. æ ¹æ®å‡è®¾åˆ†ææ¯ä¸ªå‘¨æœŸçš„å…ƒç´ æ•°ï¼Œå»ºç«‹å°†æ‰€æœ‰å…ƒç´ å¡«å…¥è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨æ ¼çš„è§„åˆ™
        4. æ ¹æ®æ‰€å»ºç«‹çš„è§„åˆ™ä»¥pythonä»£ç å½¢å¼ç¼–å†™ä¸€æ®µå°†å…ƒç´ å¡«å…¥è™šæ‹Ÿå‘¨è¡¨æ ¼çš„å‡½æ•°
        5. ç¼–å†™ä»£ç è¦æ±‚è¡¨æ ¼ä¸­å„å…ƒç´ æŒ‰ç…§ä¸»å±æ€§ä»å°åˆ°å¤§é¡ºåºæ’åˆ—ï¼Œè¡Œåˆ—å‡ä¸å¯ä¹±åº
        6. è¦æ±‚æ¯ä¸ªå…ƒç´ éƒ½å¡«å…¥è¡¨ä¸­ï¼Œæ ¹æ®å…ƒç´ å±æ€§ä¸ºæ¯ä¸ªå…ƒç´ ç¡®å®šåˆé€‚ä¸”å”¯ä¸€çš„è¡¨æ ¼ä½ç½®(row,col)
        7. å…è®¸æ¯è¡Œæˆ–æ¯åˆ—å…ƒç´ æ•°é‡ä¸åŒï¼Œä½†å¿…é¡»ä¿è¯å…ƒç´ ä½ç½®æ²¡æœ‰é‡å 
        8. å…è®¸å…ƒç´ é—´å­˜åœ¨é—´éš”ï¼Œå› ä¸ºè¡¨æ ¼ä¸­å­˜åœ¨éƒ¨åˆ†å°šæœªå‘ç°çš„å…ƒç´ 
        9. å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€ä¸æ˜¯å‚è€ƒçŠ¶æ€ï¼Œä¸ä¸€å®šæ­£ç¡®
        """

        code_requrement = f"""
        1. è¾“å…¥: 
            current_df: DataFrame # åŒ…å«å…ƒç´ æ‰€æœ‰å±æ€§ä»¥åŠå½“å‰æ‰€åœ¨è¡¨æ ¼çš„è¡Œ(row)å’Œåˆ—(col)
        2. è¾“å‡ºï¼š
            results: [(elem_name, row, col), ...] # æ‰€æœ‰å…ƒç´ ä½ç½®çš„Python list, rol å’Œ col ä¸ºæ­£æ•´æ•°
        3. æ³¨æ„å¯¼å…¥å‡½æ•°æ‰€éœ€çš„package
        """


        user_prompt = f"""
        å½“å‰é€‰æ‹©çš„ä¸»å±æ€§ä¸ºï¼š
        <attribute>
        {attribute}
        </attribute>

        ä»»åŠ¡è¦æ±‚ï¼š
        <task>
        {task}
        </task>

        ä»£ç è¦æ±‚ï¼š
        <code_requrement>
        {code_requrement}
        </code_requrement>

        ä»¥å¾€çš„å†å²è®°å½•ï¼ˆåŒ…å«å‡è®¾ã€è¯„ä»·ä¸é¢„æµ‹å…ƒç´ æˆåŠŸåŒ¹é…ç‡ï¼‰ä¸ºï¼š
        <history>
        {history.select_record()} 
        </history>

        å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€åŒ…æ‹¬ï¼š
        <state introduction>
        {self.state_introduction}
        </state introduction>

        è™šæ‹Ÿå…ƒç´ åŠå±æ€§ä¿¡æ¯ï¼š
        <elem_df>
        {table.elem_df}
        </elem_df>

        å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€ä¸º(ä¸Šä¸€è½®æ•ˆæœä¸å¥½æ—¶æ— éœ€è€ƒè™‘)ï¼š
        <state>
        {state}
        </state>


        ä½ çš„å›å¤å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š
        <reasoning>
        æ­¤å¤„ç»™å‡ºæ¨ç†è¿‡ç¨‹
        </reasoning>

        <hypothesis>
        1. è¾ƒä¸ºç²¾ç®€çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹å‡è®¾
        2. å°†å…ƒç´ å¡«å…¥è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çš„è§„åˆ™
        </hypothesis>

        <code>
        æ­¤å¤„ç»™å‡ºå‡½æ•°ä»£ç 
        </code>

        <func_name>
        å°†ä¸Šè¿°ä»£ç ä¸­å‡½æ•°çš„åç§°å†™åœ¨æ­¤å¤„
        </func_name>
        """
        
        # æ„é€ ç¬¦åˆAPIè¦æ±‚çš„messagesåˆ—è¡¨
        prompt_msgs = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        raw_response = self.get_LLM_response(prompt_msgs)

        return {
            "task": task,
            "reasoning": self.extract_content("reasoning", raw_response),
            "hypothesis": self.extract_content("hypothesis", raw_response),
            "code": self.extract_content("code", raw_response),
            "func_name": self.extract_content("func_name", raw_response),
            "raw_response": raw_response  # ä¿ç•™åŸå§‹å“åº”ä»¥é˜²è§£æå¤±è´¥
        }

class DeductionAgent(Agent):
    
    def __init__(self):
        super(DeductionAgent, self).__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        # å®šä¹‰è§’è‰²æç¤ºè¯
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä½è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨ç ”ç©¶ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®ç»™å®šçš„å‘¨æœŸå¾‹å‡è®¾å’Œå·²å¡«å…¥å…ƒç´ çš„è™šæ‹Ÿå‘¨æœŸè¡¨é¢„æµ‹æ½œåœ¨çš„æœªçŸ¥å…ƒç´ ã€‚
        """
        self.state_introduction = """
        1. å…ƒç´ è¡¨æ ¼ï¼ŒåŒ…æ‹¬å…ƒç´ åç§°ï¼ˆæ— ä»»ä½•å®é™…å«ä¹‰ï¼‰ã€å…ƒç´ å±æ€§ã€å…ƒç´ åœ¨è™šæ‹Ÿå‘¨æœŸè¡¨æ ¼çš„ä½ç½®ï¼ˆè‹¥å·²è¢«å¡«å…¥ï¼‰
        2. ä»¥è™šæ‹Ÿå…ƒç´ è¡¨æ ¼å½¢å¼å‘ˆç°çš„æ‰€æœ‰å·²è¢«å¡«å…¥è¡¨æ ¼å…ƒç´ å±æ€§å€¼
        """

    def predict_elements(self, state, hypothesis, code, history,n=1):

        task = f"""
        1. æ ¹æ®ä¸»å±æ€§çš„ç¼ºå¤±æƒ…å†µï¼Œé¢„æµ‹å¯èƒ½ç¼ºå¤±çš„å…ƒç´ ï¼Œæå‡º{n}ä¸ªä¸»å±æ€§ä¸åŒã€ä½ç½®ä¸åŒçš„æœ€æœ‰å¯èƒ½çš„æ½œåœ¨æœªçŸ¥å…ƒç´ ï¼Œå…ƒç´ å‘½åä¸ºNewElem1, NewElem2, ...
        2. æ ¹æ®æä¾›çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹å‡è®¾ã€å½“å‰çš„å…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€å’Œå†å²è®°å½•ï¼Œé¢„æµ‹{n}ä¸ªæ½œåœ¨æœªçŸ¥å…ƒç´ çš„å„å±æ€§å€¼
        3. ç»™å‡ºæ‰€æœ‰æ½œåœ¨æœªçŸ¥å…ƒç´ åœ¨å…ƒç´ å‘¨æœŸè¡¨ä¸­çš„è¡Œ(row)å’Œåˆ—(col)ï¼Œæ½œåœ¨å…ƒç´ é—´ä¸èƒ½æœ‰ä½ç½®é‡å¤
        4. å¿…é¡»ç¡®ä¿æœªçŸ¥å…ƒç´ çš„ä¸»å±æ€§å€¼ä¸¥æ ¼æŒ‰å‡åºæ’åˆ—ï¼Œä¸èƒ½æœ‰é‡å¤å€¼ã€è´Ÿå€¼å’Œç¦»ç¾¤å€¼
        5. è€ƒè™‘å†å²è®°å½•ä¸­å…ˆå‰çš„é¢„æµ‹å…ƒç´ ï¼Œè€ƒè™‘inductionä¸­ç»™çš„è¯„ä¼°ç»“æœï¼Œè¿›è¡Œæ½œåœ¨å…ƒç´ çš„æ¯”è¾ƒå’Œä¼˜åŒ–
        6. å¿…é¡»ç¡®ä¿å…ƒç´ ä¹‹é—´ä»¥åŠä¸å·²çŸ¥å…ƒç´ åœ¨å…ƒç´ å‘¨æœŸè¡¨çš„ä½ç½®æ²¡æœ‰é‡å 
        7. å‘¨æœŸè¡¨çš„æ’åˆ—å¿…é¡»éµå¾ªï¼šä¸»å±æ€§å€¼å°çš„å…ƒç´ åœ¨å·¦ä¸Šè§’ï¼Œä¸»å±æ€§å€¼å¤§çš„å…ƒç´ åœ¨å³ä¸‹è§’
        8. é¿å…ç”Ÿæˆå¤šä¸ªé«˜åº¦ç›¸ä¼¼ä¸”æ˜ å°„åˆ°åŒä¸€çœŸå®æµ‹è¯•å…ƒç´ çš„é¢„æµ‹ï¼›è‹¥å¤šä¸ªæ–°å…ƒç´ æŒ‡å‘åŒä¸€ä¸ªæµ‹è¯•ç›®æ ‡ï¼Œåªè®¡ä¸€ä¸ªè¦†ç›–ï¼Œå†—ä½™é¢„æµ‹åº”å‡å°‘
        9. æ ¹æ®æä¾›çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹å‡è®¾å’Œå¡«è¡¨å‡½æ•°ä»£ç ï¼ˆè¾“å…¥ä¸ºå…ƒç´ å±æ€§ï¼Œè¾“å‡ºä¸ºè¡¨ä¸­ä½ç½®ï¼‰ï¼Œå†™å‡ºå…¶å¯¹åº”çš„é€†å‡½æ•°ä»£ç ï¼Œé€†å‡½æ•°å‘½åä¸ºinverse_func
        10. é€†å‡½æ•°è¦æ±‚ä»¥æ½œåœ¨å…ƒç´ åœ¨å…ƒç´ å‘¨æœŸè¡¨ä¸­ä½ç½®ä¸ºè¾“å…¥ï¼Œè¾“å‡ºå…¶æ‰€æœ‰å±æ€§å€¼
        11. æ³¨æ„å…ƒç´ å±æ€§å€¼çš„å˜é‡ç±»å‹ï¼Œå¯èƒ½æ˜¯æ•°å­—ä¹Ÿå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œæ³¨æ„é¿å…ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ï¼Œæ³¨æ„æ‹¬å·çš„é—­åˆæ€§
        """

        code_requrement = f"""
        1. è¾“å…¥: 
            element_positions: [(elem_name, row, col), ...] # æ‰€æœ‰å…ƒç´ ä½ç½®çš„Python list
            current_df: å½“å‰å…ƒç´ è¡¨æ ¼DataFrame
        2. è¾“å‡ºï¼š
            results: [(elem_name, attr1, attr2, ..., row, col), ...] # æ‰€æœ‰å…ƒç´ å±æ€§ä¸ä½ç½®çš„Python list
        3. æ³¨æ„å¯¼å…¥å‡½æ•°æ‰€éœ€çš„package
        4. æ³¨æ„ä¸è¦è°ƒç”¨å¤–éƒ¨æœªå®šä¹‰çš„å‡½æ•°
        """

        user_prompt = f"""
        å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€åŒ…æ‹¬ï¼š

        <state introduction>
        {self.state_introduction}
        </state introduction>

        å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€ä¸ºï¼š
        <state>
        {state}
        </state>

        å½“å‰çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹å‡è®¾ä¸ºï¼š
        <hypothesis>
        {hypothesis}
        </hypothesis>

        å·²çŸ¥å¡«è¡¨å‡½æ•°ä¸ºï¼š
        <code>
        {code}
        </code>

        ä»»åŠ¡è¦æ±‚ï¼š
        <task>
        {task}
        </task>

        ä»£ç è¦æ±‚ï¼š
        <core_requrement>
        {code_requrement}
        </core_requrement>

        ä»¥å¾€çš„å†å²è®°å½•ï¼ˆåŒ…å«å‡è®¾ã€è¯„ä»·ä¸é¢„æµ‹å…ƒç´ æˆåŠŸåŒ¹é…ç‡ï¼‰ä¸ºï¼š
        <history>
        {history.select_record()} 
        </history>

        ä½ çš„å›å¤å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š

        <reasoning>
        æ­¤å¤„ç»™å‡ºæ¨ç†è¿‡ç¨‹
        </reasoning>

        <new_elem>
        NewElem1, row1, col1
        NewElem2, row2, col2
        ... 
        </new_elem>

        <inverse_code>
        æ­¤å¤„ç»™å‡ºä»£ç 
        </inverse_code>

        <func_name>
        å°†ä¸Šè¿°ä»£ç ä¸­å‡½æ•°çš„åç§°å†™åœ¨æ­¤å¤„
        </func_name>
        """
        
        # è·å–å¤§æ¨¡å‹å“åº”
        prompt_msgs = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        raw_response = self.get_LLM_response(prompt_msgs)

        # æå–å¹¶å¤„ç†new_elem
        new_elem_text = self.extract_content("new_elem", raw_response)
        new_elems_posi = []
        if new_elem_text:
            for line in new_elem_text.split('\n'):
                if line.strip():
                    parts = [x.strip() for x in line.split(',')]
                    if len(parts) == 3:
                        elem, row, col = parts
                        new_elems_posi.append((elem, int(row), int(col)))

        return {
            "task": task,
            "reasoning": self.extract_content("reasoning", raw_response),
            "new_elems_posi": new_elems_posi,
            "inverse_code": self.extract_content("inverse_code", raw_response),
            "func_name": self.extract_content("func_name", raw_response),
            "raw_response": raw_response
        }


class InductionAgent(Agent):
    def __init__(self):
        super(InductionAgent, self).__init__()
        # å®šä¹‰è§’è‰²æç¤ºè¯
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä½è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹è¯„å®¡ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®å¯¹åº”çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨æ ¼çŠ¶æ€è¯„ä»·å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸæ€§è§„å¾‹ï¼Œå¹¶æœ€ç»ˆåšå‡ºå†³å®šã€‚
        """
        self.state_introduction = """
        1. å…ƒç´ è¡¨æ ¼ï¼ŒåŒ…æ‹¬å…ƒç´ åç§°ï¼ˆæ— ä»»ä½•å®é™…å«ä¹‰ï¼‰ã€å…ƒç´ å±æ€§ã€å…ƒç´ åœ¨è™šæ‹Ÿå‘¨æœŸè¡¨æ ¼çš„ä½ç½®ï¼ˆè‹¥å·²è¢«å¡«å…¥ï¼‰
        2. ä»¥è™šæ‹Ÿå…ƒç´ è¡¨æ ¼å½¢å¼å‘ˆç°çš„æ‰€æœ‰å·²è¢«å¡«å…¥è¡¨æ ¼å…ƒç´ å±æ€§å€¼

        """
        self.options = {
            'P': 'pass the hypothesis',
            'A': 'adjust the hypothesis without changing main attribute',
            #'C': 'change the main attribute',
            'R': 'rollback to previous version due to constraint violation or poor performance'
        }
        
        # å®šä¹‰å†³ç­–å‡½æ•°çš„å·¥å…·æè¿°
        self.decision_tools = [
            {
                "type": "function",
                "function": {
                    "name": "make_decision",
                    "description": "æ ¹æ®åŒ¹é…ç‡å’Œå‡è®¾è¡¨ç°åšå‡ºå†³ç­–",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "match_rate": {
                                "type": "number",
                                "description": "å½“å‰åŒ¹é…ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰"
                            },
                            "previous_match_rate": {
                                "type": "number",
                                "description": "ä¹‹å‰çš„åŒ¹é…ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œç”¨äºæ¯”è¾ƒ"
                            },
                            "main_attribute_ordered": {
                                "type": "boolean",
                                "description": "ä¸»å±æ€§æ˜¯å¦ä¸¥æ ¼æœ‰åº"
                            },
                            "constraint_violation": {
                                "type": "boolean",
                                "description": "æ˜¯å¦å­˜åœ¨çº¦æŸè¿å"
                            },
                            "reasoning": {
                                "type": "string",
                                "description": "å†³ç­–æ¨ç†è¿‡ç¨‹"
                            }
                        },
                        "required": ["match_rate", "previous_match_rate", "main_attribute_ordered", "constraint_violation", "reasoning"]
                    }
                }
            }
        ]

    def make_decision(self, match_rate, previous_match_rate, main_attribute_ordered, constraint_violation, reasoning):
        """
        å†³ç­–å‡½æ•°ï¼šæ ¹æ®åŒ¹é…ç‡å’Œå‡è®¾è¡¨ç°åšå‡ºå†³ç­–
        
        Args:
            match_rate: å½“å‰åŒ¹é…ç‡
            previous_match_rate: ä¹‹å‰çš„åŒ¹é…ç‡
            main_attribute_ordered: ä¸»å±æ€§æ˜¯å¦ä¸¥æ ¼æœ‰åº
            constraint_violation: æ˜¯å¦å­˜åœ¨çº¦æŸè¿å
            reasoning: å†³ç­–æ¨ç†è¿‡ç¨‹
            
        Returns:
            dict: åŒ…å«å†³ç­–ç»“æœå’Œè¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        # å†³ç­–é€»è¾‘
        if constraint_violation and match_rate < 0.1:
            # å­˜åœ¨çº¦æŸè¿åï¼Œé€‰æ‹©å›æ»š
            decision = "R"
            decision_reason = "å­˜åœ¨çº¦æŸè¿åï¼Œéœ€è¦å›æ»š"
        elif match_rate > 0.8:
            # ä¸»å±æ€§ä¸¥æ ¼æœ‰åºä¸”åŒ¹é…ç‡>0.8ï¼Œé€‰æ‹©é€šè¿‡
            decision = "P"
            decision_reason = "ä¸»å±æ€§ä¸¥æ ¼æœ‰åºä¸”è¡¨ç°ä¼˜ç§€ï¼ˆåŒ¹é…ç‡>0.8ï¼‰ï¼Œé€šè¿‡å‡è®¾"
        elif  match_rate > 0.6:
            # ä¸»å±æ€§ä¸¥æ ¼æœ‰åºä¸”åŒ¹é…ç‡>0.6ï¼Œé€‰æ‹©é€šè¿‡
            decision = "P"
            decision_reason = "ä¸»å±æ€§ä¸¥æ ¼æœ‰åºä¸”è¡¨ç°åŠæ ¼ï¼ˆåŒ¹é…ç‡>0.6ï¼‰ï¼Œé€šè¿‡å‡è®¾"
        elif match_rate > previous_match_rate:
            # åŒ¹é…ç‡æœ‰æå‡ï¼Œé€‰æ‹©è°ƒæ•´
            decision = "A"
            decision_reason = f"åŒ¹é…ç‡æœ‰æå‡ï¼ˆä»{previous_match_rate:.3f}æå‡åˆ°{match_rate:.3f}ï¼‰ï¼Œé€‰æ‹©è°ƒæ•´"
        elif match_rate < previous_match_rate:
            # åŒ¹é…ç‡ä¸‹é™ï¼Œé€‰æ‹©å›æ»š
            decision = "R"
            decision_reason = f"åŒ¹é…ç‡ä¸‹é™ï¼ˆä»{previous_match_rate:.3f}ä¸‹é™åˆ°{match_rate:.3f}ï¼‰ï¼Œé€‰æ‹©å›æ»š"
        else:
            # åŒ¹é…ç‡æ— å˜åŒ–ï¼Œæ ¹æ®å½“å‰è¡¨ç°å†³å®š
            if match_rate > 0.3:
                decision = "A"
                decision_reason = "åŒ¹é…ç‡æ— å˜åŒ–ä½†è¡¨ç°å°šå¯ï¼Œé€‰æ‹©è°ƒæ•´"
            else:
                decision = "R"
                decision_reason = "åŒ¹é…ç‡æ— å˜åŒ–ä¸”è¡¨ç°ä¸ä½³ï¼Œé€‰æ‹©å›æ»š"
        
        return {
            "decision": decision,
            "decision_reason": decision_reason,
            "match_rate": match_rate,
            "previous_match_rate": previous_match_rate,
            "main_attribute_ordered": main_attribute_ordered,
            "constraint_violation": constraint_violation,
            "reasoning": reasoning
        }

    def evaluate_hypothesis(self, state, hypothesis, matched_elem_str, match_rate, avg_matched_score, previous_match_rate=0.0):
        task = """
        1. è¯„ä»·å½“å‰çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹æ˜¯å¦ç¬¦åˆçº¦æŸæ¡ä»¶ï¼š
           - ä¸»å±æ€§æ˜¯å¦ä¸¥æ ¼æŒ‰å‡åºæ’åˆ—ï¼Œé™¤æ–°æ·»åŠ çš„å…ƒç´ å¤–ä¸èƒ½æœ‰é‡å¤å€¼
           - å…ƒç´ æ˜¯å¦æŒ‰ä¸»å±æ€§å€¼ä»å°åˆ°å¤§ä¸¥æ ¼é¡ºåºæ’åˆ—
           - è¡Œåˆ—æ˜¯å¦æŒ‰ç…§ä¸»å±æ€§å€¼ä¸¥æ ¼é€’å¢æ’åˆ—
        2. è¯„ä»·å½“å‰çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨éä¸»å±æ€§æ˜¯å¦å‘ˆç°æ˜æ˜¾æŒ‰è¡Œå‘¨æœŸå˜åŒ–ã€æŒ‰åˆ—ç›¸ä¼¼æˆ–æ¸å˜çš„è§„å¾‹
        3. è¯„ä»·æ‰€é¢„æµ‹æ½œåœ¨å…ƒç´ ï¼ˆä»¥NewElemå‘½åï¼‰çš„ä¸æµ‹è¯•é›†åŒ¹é…æƒ…å†µï¼Œä¾‹å¦‚å¯¼è‡´æœ‰ä¸åŒ¹é…çš„å…ƒç´ çš„å¯èƒ½åŸå› 
        4. è¯„ä¼°å½“å‰å‡è®¾çš„æ•´ä½“è¡¨ç°ï¼ˆåŒ¹é…ç‡ã€è§„å¾‹æ€§ç­‰ï¼‰
        5. è¯„ä¼°æ˜¯å¦æœ‰å‡ºç°å¤šä¸ªæ–°å…ƒç´ ä¸åŒä¸€ä¸ªç›®æ ‡åŒ¹é…çš„æƒ…å†µï¼Œç»™å‡ºæé†’
        6. ä½¿ç”¨make_decisionå‡½æ•°åšå‡ºæœ€ç»ˆå†³ç­–ï¼š
           - å¦‚æœè¿èƒŒçº¦æŸæ¡ä»¶ï¼ˆå¦‚é™¤æ–°æ·»åŠ çš„å…ƒç´ å¤–ä¸»å±æ€§é‡å¤ã€æ’åºé”™è¯¯ç­‰ï¼‰ï¼Œé€‰æ‹©Rï¼ˆå›æ»šï¼‰
           - å¦‚æœåŒ¹é…ç‡æœ‰æå‡ï¼Œå³ä½¿è¡¨ç°ä»ä¸å¤Ÿç†æƒ³ï¼Œå°å¹…è°ƒæ•´å‡è®¾ï¼Œé€‰æ‹©Aï¼ˆè°ƒæ•´ï¼‰
           - å¦‚æœå½“å‰å‡è®¾è¡¨ç°å˜å¾—æ›´å·®ï¼ˆåŒ¹é…ç‡ä¸‹é™ï¼Œè§„å¾‹æ€§ä¸æ˜æ˜¾ï¼‰ï¼Œé€‰æ‹©Rï¼ˆå›æ»šï¼‰
           - å¦‚æœå‡è®¾é€šè¿‡ä¸”è¡¨ç°è‰¯å¥½ï¼ˆåŠæ ¼ï¼šåŒ¹é…ç‡>0.6,è‰¯å¥½ï¼šåŒ¹é…ç‡>0.8ï¼Œä¸»å±æ€§ä¸¥æ ¼æœ‰åºï¼‰ï¼Œé€‰æ‹©Pï¼ˆé€šè¿‡ï¼‰
        """

        user_prompt = f"""
        å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€åŒ…æ‹¬ï¼š
        <state introduction>
        {self.state_introduction}
        </state introduction>

        å½“å‰è™šæ‹Ÿå…ƒç´ å‘¨æœŸè¡¨çŠ¶æ€ä¸ºï¼š
        <state>
        {state}
        </state>

        å½“å‰çš„è™šæ‹Ÿå…ƒç´ å‘¨æœŸå¾‹å‡è®¾ä¸ºï¼š
        <hypothesis>
        {hypothesis}
        </hypothesis>

        å¹³å‡åŒ¹é…åˆ†æ•°ï¼š
        <avg_matched_score>
        {avg_matched_score}
        </avg_matched_score>

        ä¸æµ‹è¯•é›†å…ƒç´ èƒ½åŒ¹é…ä¸Šçš„æ½œåœ¨æ–°å…ƒç´ ï¼š
        <matched_elem>
        {matched_elem_str}
        åŒ¹é…æˆåŠŸç‡{match_rate}
        </matched_elem>

        ä¹‹å‰çš„åŒ¹é…ç‡ï¼š
        <previous_match_rate>
        {previous_match_rate}
        </previous_match_rate>

        ä»»åŠ¡è¦æ±‚ï¼š
        <task>
        {task}
        </task>

        å†³ç­–é€‰é¡¹ï¼š
        <options>
        {self.options}
        </options>

        è¯·ä½¿ç”¨make_decisionå‡½æ•°æ¥åšå‡ºæœ€ç»ˆå†³ç­–ã€‚ä½ çš„å›å¤å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š

        <reasoning>
        æ­¤å¤„ç»™å‡ºæ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¯¹çº¦æŸæ¡ä»¶ã€è§„å¾‹æ€§å’ŒåŒ¹é…ç‡çš„è¯„ä¼°
        </reasoning>

        <evaluation>
        æ­¤å¤„ç»™å‡ºå¯¹å‡è®¾çš„è¯„ä»·ä»¥åŠä¿®æ”¹å»ºè®®
        </evaluation>

        <constraint_analysis>
        æ­¤å¤„åˆ†æä¸»å±æ€§æ˜¯å¦ä¸¥æ ¼æœ‰åºä»¥åŠæ˜¯å¦å­˜åœ¨çº¦æŸè¿å
        </constraint_analysis>

        <decision_call>
        è°ƒç”¨make_decisionå‡½æ•°ï¼Œä¼ å…¥ä»¥ä¸‹å‚æ•°ï¼š
        - match_rate: {match_rate}
        - previous_match_rate: {previous_match_rate}
        - main_attribute_ordered: [æ ¹æ®çº¦æŸåˆ†æç»“æœå¡«å†™trueæˆ–false]
        - constraint_violation: [æ ¹æ®çº¦æŸåˆ†æç»“æœå¡«å†™trueæˆ–false]
        - reasoning: [æ€»ç»“ä½ çš„æ¨ç†è¿‡ç¨‹]
        </decision_call>
        """
        
        # è·å–å¤§æ¨¡å‹å“åº”
        prompt_msgs = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # ä½¿ç”¨function callingè·å–å“åº”
        raw_response = self.get_LLM_response_with_tools(prompt_msgs, self.decision_tools)

        # æå–çº¦æŸåˆ†æç»“æœ
        constraint_analysis = self.extract_content("constraint_analysis", raw_response)
        
        # æ ¹æ®çº¦æŸåˆ†æç»“æœè°ƒç”¨å†³ç­–å‡½æ•°
        main_attribute_ordered = "true" in constraint_analysis.lower() if constraint_analysis else False
        constraint_violation = "violation" in constraint_analysis.lower() or "è¿å" in constraint_analysis if constraint_analysis else False
        
        # è°ƒç”¨å†³ç­–å‡½æ•°
        decision_result = self.make_decision(
            match_rate=match_rate,
            previous_match_rate=previous_match_rate,
            main_attribute_ordered=main_attribute_ordered,
            constraint_violation=constraint_violation,
            reasoning=self.extract_content("reasoning", raw_response) or ""
        )

        return {
            "task": task,
            "reasoning": self.extract_content("reasoning", raw_response),
            "evaluation": self.extract_content("evaluation", raw_response),
            "constraint_analysis": constraint_analysis,
            "decision": decision_result["decision"],
            "decision_reason": decision_result["decision_reason"],
            "decision_details": decision_result,
            "raw_response": raw_response
        }
    
class RecordAgent(Agent):
    def __init__(self):
        super(RecordAgent, self).__init__()
        # å®šä¹‰è§’è‰²æç¤ºè¯
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªå‡è®¾æ€»ç»“è¯„ä»·ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®å†å²è®°å½•é€‰æ‹©å¹¶æ€»ç»“æœ€ä¼˜å‡è®¾ã€‚
        """
        self.history_introduction = """
        åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå‡è®¾ï¼Œæ¯ä¸ªå‡è®¾åŒ…å«å‡è®¾ã€è¯„ä»·ä¸é¢„æµ‹å…ƒç´ æˆåŠŸåŒ¹é…ç‡ï¼Œä»¥åŠä¸»å±æ€§ä¿¡æ¯ã€‚
        
        """

    def merge_records(self, records):
        task = '''
        1. æ ¹æ®æä¾›çš„æœ€ä¼˜å‡è®¾ï¼Œä»¥åŠæœ€åçš„å‡è®¾æ€»ç»“å¹¶ä¼˜åŒ–å‡è®¾ã€‚
        2. å‡è®¾å°½å¯èƒ½æ¸…æ™°ï¼Œä¾¿äºåç»­è½¬åŒ–ä¸ºå½¢å¼è¯­è¨€ï¼Œå¦‚pythonä»£ç ã€‚
        3. è¾“å‡ºçš„æ ¼å¼ä¸å•ä¸ªå†å²è®°å½•ä¸€è‡´ï¼ŒåŒ…æ‹¬å‡è®¾ã€è¯„ä»·ä¸é¢„æµ‹å…ƒç´ æˆåŠŸåŒ¹é…ç‡ï¼Œä»¥åŠä¸»å±æ€§ã€ä¸»å±æ€§æ˜¯å¦å‡åºã€‚
        '''
        user_prompt = f'''
        å½“å‰çš„å†å²è®°å½•ä¸ºï¼š
        <history introduction>
        {self.history_introduction}
        </history introduction>
        
        
        ä»»åŠ¡è¦æ±‚ï¼š
        <task>
        {task}
        </task>
        
        å†å²è®°å½•ï¼š
        <history>
        {records}
        </history>

        '''
        prompt_msgs = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        json_response = self.get_LLM_structured_response(prompt_msgs)
        return json_response

